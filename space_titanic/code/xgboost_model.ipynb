{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.conda\\envs\\tensor4\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import feat_eng\n",
    "import importlib\n",
    "import model\n",
    "import optuna\n",
    "from functools import partial\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(feat_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'..\\data\\train.csv')\n",
    "test = pd.read_csv(r'..\\data\\test.csv')\n",
    "X_train, X_test, y_train, y_test, X_transformed = feat_eng.feat_eng(data, train=True)\n",
    "blind = feat_eng.feat_eng(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns='Transported')\n",
    "y = data['Transported']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_svm_accuracy(X_transformed, y, feature_names):\n",
    "    \"\"\"\n",
    "    Calculate SVM prediction accuracy for each pair of features using PyTorch for GPU acceleration.\n",
    "    \n",
    "    :param X_transformed: Preprocessed and transformed features (after One-Hot Encoding and Scaling)\n",
    "    :param y: True target values (e.g., 'Transported' in your dataset)\n",
    "    :param feature_names: List of feature names for the transformed data\n",
    "    :return: DataFrame containing accuracy score for each pair of features\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from joblib import Parallel, delayed\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Convert to numpy arrays if needed\n",
    "    X_array = X_transformed.values if hasattr(X_transformed, 'values') else X_transformed\n",
    "    feature_names = list(X_transformed.columns) if hasattr(X_transformed, 'columns') else feature_names\n",
    "    \n",
    "    # Create a consistent train/test split once\n",
    "    X_indices = np.arange(X_array.shape[0])\n",
    "    train_indices, test_indices = train_test_split(X_indices, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # This function will process batches of feature pairs to maximize GPU utilization\n",
    "    def process_feature_pairs_batch(feature_pairs_batch):\n",
    "        results = []\n",
    "        \n",
    "        # Move data to GPU in batches\n",
    "        X_batch_train = X_array[train_indices]\n",
    "        X_batch_test = X_array[test_indices]\n",
    "        y_batch_train = y[train_indices]\n",
    "        y_batch_test = y[test_indices]\n",
    "        \n",
    "        # Convert to PyTorch tensors and move to GPU\n",
    "        X_batch_train_tensor = torch.tensor(X_batch_train, dtype=torch.float32, device=device)\n",
    "        X_batch_test_tensor = torch.tensor(X_batch_test, dtype=torch.float32, device=device)\n",
    "        \n",
    "        for i, j in feature_pairs_batch:\n",
    "            # Extract feature pairs using PyTorch indexing\n",
    "            X_pair_train = X_batch_train_tensor[:, [i, j]].cpu().numpy()\n",
    "            X_pair_test = X_batch_test_tensor[:, [i, j]].cpu().numpy()\n",
    "            \n",
    "            # Train and evaluate (SVC on CPU as PyTorch doesn't have direct SVM implementation)\n",
    "            svm = SVC(kernel='linear', C=1.0, cache_size=1000, max_iter=1000)\n",
    "            svm.fit(X_pair_train, y_batch_train)\n",
    "            y_pred = svm.predict(X_pair_test)\n",
    "            accuracy = accuracy_score(y_batch_test, y_pred)\n",
    "            \n",
    "            results.append({\n",
    "                'Feature 1': feature_names[i],\n",
    "                'Feature 2': feature_names[j],\n",
    "                'Accuracy': accuracy\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # Generate all feature pairs\n",
    "    num_features = X_array.shape[1]\n",
    "    feature_pairs = [(i, j) for i in range(num_features) for j in range(i+1, num_features)]\n",
    "    \n",
    "    # Process in batches to better utilize GPU memory\n",
    "    batch_size = 200  # Adjust batch size based on your GPU memory\n",
    "    accuracy_results = []\n",
    "    \n",
    "    for i in range(0, len(feature_pairs), batch_size):\n",
    "        batch_pairs = feature_pairs[i:i+batch_size]\n",
    "        # Process each batch in parallel\n",
    "        batch_results = process_feature_pairs_batch(batch_pairs)\n",
    "        accuracy_results.extend(batch_results)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Processed {min(i+batch_size, len(feature_pairs))}/{len(feature_pairs)} feature pairs\")\n",
    "    \n",
    "    # Convert results to DataFrame and sort by accuracy (descending)\n",
    "    accuracy_df = pd.DataFrame(accuracy_results)\n",
    "    accuracy_df = accuracy_df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return accuracy_df\n",
    "\n",
    "# Usage\n",
    "accuracy_df = calculate_svm_accuracy(X_transformed, y_train, X_transformed.columns)\n",
    "# print(accuracy_df.head(10))  # Show top 10 feature pairs by accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.conda\\envs\\tensor4\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, using CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Force GPU usage and print device information\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        # Set this to prevent TF from grabbing all GPU memory if TF is installed\n",
    "    import os\n",
    "    os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow detected 1 GPU(s)\n",
      " - /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "def calculate_svm_accuracy(X_transformed, y, feature_names):\n",
    "    \"\"\"\n",
    "    Calculate SVM prediction accuracy for each pair of features using TensorFlow for GPU acceleration.\n",
    "    \n",
    "    :param X_transformed: Preprocessed and transformed features (after One-Hot Encoding and Scaling)\n",
    "    :param y: True target values (e.g., 'Transported' in your dataset)\n",
    "    :param feature_names: List of feature names for the transformed data\n",
    "    :return: DataFrame containing accuracy score for each pair of features\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from joblib import Parallel, delayed\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(f\"TensorFlow detected {len(gpus)} GPU(s)\")\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            print(f\" - {gpu.name}\")\n",
    "    \n",
    "    # Convert to numpy arrays if needed\n",
    "    X_array = X_transformed.values if hasattr(X_transformed, 'values') else X_transformed\n",
    "    feature_names = list(X_transformed.columns) if hasattr(X_transformed, 'columns') else feature_names\n",
    "    \n",
    "    # Create a consistent train/test split once\n",
    "    X_indices = np.arange(X_array.shape[0])\n",
    "    train_indices, test_indices = train_test_split(X_indices, test_size=0.3, random_state=42)\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    # Convert y to one-hot encoding if needed (for multi-class problems)\n",
    "    if len(np.unique(y)) > 2:\n",
    "        # For multi-class\n",
    "        num_classes = len(np.unique(y))\n",
    "        y_train_tf = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "        y_test_tf = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "    else:\n",
    "        # For binary classification\n",
    "        y_train_tf = y_train\n",
    "        y_test_tf = y_test\n",
    "    \n",
    "    def process_feature_pair(i, j):\n",
    "        # Extract feature pair\n",
    "        X_pair_train = X_array[train_indices][:, [i, j]]\n",
    "        X_pair_test = X_array[test_indices][:, [i, j]]\n",
    "        \n",
    "        # Define a TensorFlow model with a structure equivalent to a linear SVM\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(1, activation='linear', input_shape=(2,), \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        ])\n",
    "        \n",
    "        # Use hinge loss for SVM-like behavior\n",
    "        if len(np.unique(y)) == 2:\n",
    "            # Binary classification\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                loss=tf.keras.losses.Hinge(),\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            # Convert labels to -1 and 1 for hinge loss\n",
    "            y_train_svm = np.where(y_train_tf == 0, -1, 1)\n",
    "            \n",
    "        else:\n",
    "            # Multi-class classification\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                loss=tf.keras.losses.CategoricalHinge(),\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            y_train_svm = y_train_tf\n",
    "            \n",
    "        # Train the model with early stopping\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "        model.fit(\n",
    "            X_pair_train, y_train_svm, \n",
    "            epochs=50, \n",
    "            batch_size=32, \n",
    "            verbose=0,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stop]\n",
    "        )\n",
    "        \n",
    "        # Evaluate the model\n",
    "        _, accuracy = model.evaluate(X_pair_test, y_test_tf if len(np.unique(y)) > 2 else np.where(y_test_tf == 0, -1, 1), verbose=0)\n",
    "        \n",
    "        # Clear TensorFlow session to free memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        return {\n",
    "            'Feature 1': feature_names[i],\n",
    "            'Feature 2': feature_names[j],\n",
    "            'Accuracy': accuracy\n",
    "        }\n",
    "    \n",
    "    # Generate all feature pairs\n",
    "    num_features = X_array.shape[1]\n",
    "    feature_pairs = [(i, j) for i in range(num_features) for j in range(i+1, num_features)]\n",
    "    \n",
    "    # Process feature pairs in batches\n",
    "    batch_size = 50  # Adjust based on your available memory\n",
    "    accuracy_results = []\n",
    "    \n",
    "    for i in range(0, len(feature_pairs), batch_size):\n",
    "        batch_pairs = feature_pairs[i:i+batch_size]\n",
    "        batch_results = Parallel(n_jobs=4)(  # Limited parallelism to avoid GPU contention\n",
    "            delayed(process_feature_pair)(i, j) for i, j in batch_pairs\n",
    "        )\n",
    "        accuracy_results.extend(batch_results)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Processed {min(i+batch_size, len(feature_pairs))}/{len(feature_pairs)} feature pairs\")\n",
    "    \n",
    "    # Convert results to DataFrame and sort by accuracy (descending)\n",
    "    accuracy_df = pd.DataFrame(accuracy_results)\n",
    "    accuracy_df = accuracy_df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    return accuracy_df\n",
    "\n",
    "# Usage\n",
    "accuracy_df = calculate_svm_accuracy(X_transformed, y_train, X_transformed.columns)\n",
    "# print(accuracy_df.head(10))  # Show top 10 feature pairs by accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize( partial(model.objective, X_train=X_train, y_train=y_train), n_trials=15, show_progress_bar=True)\n",
    "\n",
    "# Get best parameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Add fixed parameters to best_params\n",
    "best_params.update({\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['error', 'auc'],\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'device': 'cuda',\n",
    "    'seed': 42\n",
    "})\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBest parameters:\", best_params)\n",
    "print(f\"Best CV score: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "final_model = xgb.train(\n",
    "    best_params,\n",
    "    dtrain,\n",
    "    num_boost_round=10000,\n",
    "    early_stopping_rounds=50,\n",
    "    evals=[(dval, 'validation')],\n",
    "    verbose_eval=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data['VIP']==True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['RoomService','FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "(data['sp_cols']==True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblind = xgb.DMatrix(blind)\n",
    "y_pred_prob = final_model.predict(dblind)\n",
    "# Calculate the accuracy\n",
    "predicted_y  = (y_pred_prob  > 0.5).astype(int)\n",
    "\n",
    "sub = pd.merge(test[['PassengerId']], pd.DataFrame(predicted_y), left_index=True, right_index=True)#.to_csv('data/ak_submission.csv', index=False)\n",
    "sub = sub.rename(columns={0:'Transported'})\n",
    "sub['Transported'] = sub['Transported'].map({0:False, 1:True})\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"submissionxgboost_{timestamp}.csv\"\n",
    "filepath = os.path.join(r\"../submissions\", filename)  # Cross-platform\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "sub.to_csv(filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
