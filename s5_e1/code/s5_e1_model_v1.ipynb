{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LogitRegression' from 'sklearn.linear_model' (c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\linear_model\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mydata_profiling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogitRegression\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcb\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscalecast\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mForecaster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Forecaster\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LogitRegression' from 'sklearn.linear_model' (c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\linear_model\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import optuna\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import holidays \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import catboost as cb\n",
    "from scalecast.Forecaster import Forecaster\n",
    "\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "\n",
    "import time\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "sklearn.set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{datetime.date(2025, 1, 1): \"New Year's Day\", datetime.date(2025, 5, 26): 'Memorial Day', datetime.date(2025, 6, 19): 'Juneteenth National Independence Day', datetime.date(2025, 7, 4): 'Independence Day', datetime.date(2025, 9, 1): 'Labor Day', datetime.date(2025, 11, 11): 'Veterans Day', datetime.date(2025, 11, 27): 'Thanksgiving', datetime.date(2025, 12, 25): 'Christmas Day', datetime.date(2025, 1, 20): 'Martin Luther King Jr. Day', datetime.date(2025, 2, 17): \"Washington's Birthday\", datetime.date(2025, 10, 13): 'Columbus Day'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "holidays.US(years=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "blind = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_holiday(row):\n",
    "    \"\"\"\n",
    "    Determines if a given date is a holiday in the specified country.\n",
    "    Optimized for Canada, Finland, and Italy.\n",
    "    \n",
    "    Parameters:\n",
    "    row: pandas Series containing 'country' and 'date' columns\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the date is a holiday in the given country, False otherwise\n",
    "    \"\"\"\n",
    "    country = row['country']\n",
    "    date = pd.to_datetime(row['date']).date()  # Convert to date only\n",
    "\n",
    "    country_mapping = {\n",
    "        'Canada': holidays.CA(),\n",
    "        'Finland': holidays.FI(),\n",
    "        'Italy': holidays.IT(), \n",
    "        'Kenya': holidays.KE(),\n",
    "        'Norway': holidays.NO(),\n",
    "        'Singapore': holidays.SG(),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get the holidays object for the country\n",
    "        if country in country_mapping:\n",
    "            country_holidays = country_mapping[country]\n",
    "            return date in country_holidays\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking holiday for {country} on {date}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create a cache of holiday objects to improve performance\n",
    "def initialize_holiday_detection(df):\n",
    "    \"\"\"\n",
    "    Initialize holiday detection by creating holiday objects for all unique years in the dataset.\n",
    "    This improves performance by avoiding repeated creation of holiday objects.\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame containing 'date' column\n",
    "    \"\"\"\n",
    "    years = pd.to_datetime(df['date']).dt.year.unique()\n",
    "    holiday_cache = {\n",
    "        'Canada': {year: holidays.CA(years=year) for year in years},\n",
    "        'Finland': {year: holidays.FI(years=year) for year in years},\n",
    "        'Italy': {year: holidays.IT(years=year) for year in years}, \n",
    "        'Kenya': {year: holidays.KE(years=year) for year in years},\n",
    "        'Norway': {year: holidays.NO(years=year) for year in years},\n",
    "        'Singapore': {year: holidays.SG(years=year) for year in years}\n",
    "\n",
    "    }\n",
    "    \n",
    "    def is_holiday_cached(row):\n",
    "        country = row['country']\n",
    "        date = pd.to_datetime(row['date']).date()\n",
    "        year = date.year\n",
    "        \n",
    "        try:\n",
    "            if country in holiday_cache:\n",
    "                return date in holiday_cache[country][year]\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking holiday for {country} on {date}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return is_holiday_cached\n",
    "\n",
    "# Example usage:\n",
    "# First initialize the cached version\n",
    "holiday_checker = initialize_holiday_detection(train)\n",
    "\n",
    "# Then apply it to create the holiday flag\n",
    "train['is_holiday'] = train.apply(holiday_checker, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (195610, 18)\n",
      "y_train shape: (195610,)\n",
      "X_test shape: (34520, 18)\n",
      "y_test shape: (34520,)\n"
     ]
    }
   ],
   "source": [
    "def feature_eng(df):\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Extract date features\n",
    "    \n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "\n",
    "    # Encode categorical features\n",
    "    # Drop missing values and irrelevant columns\n",
    "    imputer = SimpleImputer(strategy=\"mean\")  # Change to \"mean\", \"most_frequent\", or \"constant\"\n",
    "    scaler = MinMaxScaler()\n",
    "    df = df.set_index('id')\n",
    "    df = df.drop(columns=['date'])  # Dropping columns that are not useful for modeling\n",
    "    # Separate features and target\n",
    "    try: \n",
    "        df[\"num_sold\"] = imputer.fit_transform(df[[\"num_sold\"]])\n",
    "       # df[\"num_sold\"] = scaler.fit_transform(df[[\"num_sold\"]])\n",
    "\n",
    "    except: pass\n",
    "    try: \n",
    "        X = df.drop(columns=['num_sold'])\n",
    "        X = pd.get_dummies(X)\n",
    "\n",
    "    except:\n",
    "        X=pd.get_dummies(df)\n",
    "    try:\n",
    "        y = df['num_sold']\n",
    "    except:\n",
    "        y = None\n",
    "    # Scale features\n",
    "    #X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    return df, X,  y\n",
    "\n",
    "# Feature engineering\n",
    "df, X, y = feature_eng(train)\n",
    "\n",
    "# Splitting the data\n",
    "SPLIT = 0.85\n",
    "split_index = int(SPLIT * len(X))\n",
    "\n",
    "X_train = X[:split_index]\n",
    "y_train = y[:split_index]\n",
    "X_test = X[split_index:]\n",
    "y_test = y[split_index:]\n",
    "\n",
    "# Verify shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>country_Canada</th>\n",
       "      <th>country_Finland</th>\n",
       "      <th>country_Italy</th>\n",
       "      <th>country_Kenya</th>\n",
       "      <th>country_Norway</th>\n",
       "      <th>country_Singapore</th>\n",
       "      <th>store_Discount Stickers</th>\n",
       "      <th>store_Premium Sticker Mart</th>\n",
       "      <th>store_Stickers for Less</th>\n",
       "      <th>product_Holographic Goose</th>\n",
       "      <th>product_Kaggle</th>\n",
       "      <th>product_Kaggle Tiers</th>\n",
       "      <th>product_Kerneler</th>\n",
       "      <th>product_Kerneler Dark Mode</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195605</th>\n",
       "      <td>False</td>\n",
       "      <td>348</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195606</th>\n",
       "      <td>False</td>\n",
       "      <td>348</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195607</th>\n",
       "      <td>False</td>\n",
       "      <td>348</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195608</th>\n",
       "      <td>False</td>\n",
       "      <td>348</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195609</th>\n",
       "      <td>False</td>\n",
       "      <td>348</td>\n",
       "      <td>12</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195610 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_holiday  day_of_year  month  year  country_Canada  country_Finland  \\\n",
       "id                                                                              \n",
       "0             True            1      1  2010            True            False   \n",
       "1             True            1      1  2010            True            False   \n",
       "2             True            1      1  2010            True            False   \n",
       "3             True            1      1  2010            True            False   \n",
       "4             True            1      1  2010            True            False   \n",
       "...            ...          ...    ...   ...             ...              ...   \n",
       "195605       False          348     12  2015           False            False   \n",
       "195606       False          348     12  2015           False            False   \n",
       "195607       False          348     12  2015           False            False   \n",
       "195608       False          348     12  2015           False            False   \n",
       "195609       False          348     12  2015           False            False   \n",
       "\n",
       "        country_Italy  country_Kenya  country_Norway  country_Singapore  \\\n",
       "id                                                                        \n",
       "0               False          False           False              False   \n",
       "1               False          False           False              False   \n",
       "2               False          False           False              False   \n",
       "3               False          False           False              False   \n",
       "4               False          False           False              False   \n",
       "...               ...            ...             ...                ...   \n",
       "195605           True          False           False              False   \n",
       "195606           True          False           False              False   \n",
       "195607           True          False           False              False   \n",
       "195608           True          False           False              False   \n",
       "195609           True          False           False              False   \n",
       "\n",
       "        store_Discount Stickers  store_Premium Sticker Mart  \\\n",
       "id                                                            \n",
       "0                          True                       False   \n",
       "1                          True                       False   \n",
       "2                          True                       False   \n",
       "3                          True                       False   \n",
       "4                          True                       False   \n",
       "...                         ...                         ...   \n",
       "195605                    False                       False   \n",
       "195606                    False                       False   \n",
       "195607                    False                       False   \n",
       "195608                    False                       False   \n",
       "195609                    False                       False   \n",
       "\n",
       "        store_Stickers for Less  product_Holographic Goose  product_Kaggle  \\\n",
       "id                                                                           \n",
       "0                         False                       True           False   \n",
       "1                         False                      False            True   \n",
       "2                         False                      False           False   \n",
       "3                         False                      False           False   \n",
       "4                         False                      False           False   \n",
       "...                         ...                        ...             ...   \n",
       "195605                     True                       True           False   \n",
       "195606                     True                      False            True   \n",
       "195607                     True                      False           False   \n",
       "195608                     True                      False           False   \n",
       "195609                     True                      False           False   \n",
       "\n",
       "        product_Kaggle Tiers  product_Kerneler  product_Kerneler Dark Mode  \n",
       "id                                                                          \n",
       "0                      False             False                       False  \n",
       "1                      False             False                       False  \n",
       "2                       True             False                       False  \n",
       "3                      False              True                       False  \n",
       "4                      False             False                        True  \n",
       "...                      ...               ...                         ...  \n",
       "195605                 False             False                       False  \n",
       "195606                 False             False                       False  \n",
       "195607                  True             False                       False  \n",
       "195608                 False              True                       False  \n",
       "195609                 False             False                        True  \n",
       "\n",
       "[195610 rows x 18 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0         752.527382\n",
       "1         973.000000\n",
       "2         906.000000\n",
       "3         423.000000\n",
       "4         491.000000\n",
       "             ...    \n",
       "195605    150.000000\n",
       "195606    874.000000\n",
       "195607    717.000000\n",
       "195608    453.000000\n",
       "195609    438.000000\n",
       "Name: num_sold, Length: 195610, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "195610     170.0\n",
       "195611    1047.0\n",
       "195612     981.0\n",
       "195613     513.0\n",
       "195614     492.0\n",
       "           ...  \n",
       "230125     466.0\n",
       "230126    2907.0\n",
       "230127    2299.0\n",
       "230128    1242.0\n",
       "230129    1622.0\n",
       "Name: num_sold, Length: 34520, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 248.1475\n",
      "MAPE: 3.0422\n",
      "Mean Squared Error: 118420.2639\n",
      "R-squared: 0.6478\n",
      "Coefficients: [ 3.77165272e+01  8.10233613e-02 -3.42425231e+00 -6.81727755e+00\n",
      "  7.79357301e+01 -9.81025856e+00 -2.14775054e+02 -6.50285148e+02\n",
      "  7.08026730e+02  8.89080004e+01 -3.08593593e+02  2.22084217e+02\n",
      "  8.65093759e+01 -4.56236057e+02  4.90390358e+02  2.69673676e+02\n",
      " -1.99475825e+02 -1.04352152e+02]\n",
      "Intercept: 14490.864200642352\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "# Train the model using training data\n",
    "model.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test.values)\n",
    "\n",
    "# Evaluate the model using common metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}\")\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "# Optionally: You can get the coefficients and intercept of the linear regression model\n",
    "print(f\"Coefficients: {model.coef_}\")\n",
    "print(f\"Intercept: {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:26:17,001] A new study created in memory with name: no-name-ab083a5c-0b07-4b12-ba43-df4ce3f5a67e\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a84edd58df545ae9d9a9e99fd0bd629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting Trial 1 out of 15\n",
      "✅ Trial 1 completed in 2.7s\n",
      "   MAPE: 1.8820, MAE: 110.6834, RMSE: 147.2241\n",
      "[I 2025-01-30 13:26:19,756] Trial 0 finished with value: 1.8819735792729888 and parameters: {'n_estimators': 76, 'learning_rate': 0.021529971531891543, 'max_depth': 15, 'subsample': 0.9551041287765066, 'colsample_bytree': 0.9948239312763494, 'reg_alpha': 6.937097660895436, 'reg_lambda': 0.3382793882593333, 'min_child_weight': 2, 'gamma': 2.248265238783105}. Best is trial 0 with value: 1.8819735792729888.\n",
      "🔄 Starting Trial 2 out of 15\n",
      "✅ Trial 2 completed in 3.6s\n",
      "   MAPE: 0.6619, MAE: 75.7065, RMSE: 120.6409\n",
      "[I 2025-01-30 13:26:23,392] Trial 1 finished with value: 0.6618970142189052 and parameters: {'n_estimators': 204, 'learning_rate': 0.04510335348442594, 'max_depth': 15, 'subsample': 0.8138140076129938, 'colsample_bytree': 0.5445631689210862, 'reg_alpha': 0.6993973423962253, 'reg_lambda': 5.868284764973139, 'min_child_weight': 5, 'gamma': 1.67479363594156}. Best is trial 1 with value: 0.6618970142189052.\n",
      "🔄 Starting Trial 3 out of 15\n",
      "✅ Trial 3 completed in 2.5s\n",
      "   MAPE: 0.4938, MAE: 71.4803, RMSE: 118.2657\n",
      "[I 2025-01-30 13:26:25,915] Trial 2 finished with value: 0.49384729822835427 and parameters: {'n_estimators': 180, 'learning_rate': 0.03813295073621798, 'max_depth': 9, 'subsample': 0.9191724185897566, 'colsample_bytree': 0.7075452033996124, 'reg_alpha': 5.947573415616664, 'reg_lambda': 1.2378338339639383, 'min_child_weight': 1, 'gamma': 1.0615367596491594}. Best is trial 2 with value: 0.49384729822835427.\n",
      "🔄 Starting Trial 4 out of 15\n",
      "✅ Trial 4 completed in 4.3s\n",
      "   MAPE: 0.9795, MAE: 81.8204, RMSE: 117.7389\n",
      "[I 2025-01-30 13:26:30,233] Trial 3 finished with value: 0.9794565973458151 and parameters: {'n_estimators': 157, 'learning_rate': 0.017973853047482086, 'max_depth': 17, 'subsample': 0.6607822382030206, 'colsample_bytree': 0.800396421194207, 'reg_alpha': 0.22246301609765964, 'reg_lambda': 3.3530329039792144, 'min_child_weight': 3, 'gamma': 1.8742756534231697}. Best is trial 2 with value: 0.49384729822835427.\n",
      "🔄 Starting Trial 5 out of 15\n",
      "✅ Trial 5 completed in 1.8s\n",
      "   MAPE: 0.7344, MAE: 77.8177, RMSE: 120.6977\n",
      "[I 2025-01-30 13:26:32,032] Trial 4 finished with value: 0.7344324690365572 and parameters: {'n_estimators': 187, 'learning_rate': 0.08153631904086102, 'max_depth': 5, 'subsample': 0.8318067143841206, 'colsample_bytree': 0.7449320440824349, 'reg_alpha': 1.2346258043521456, 'reg_lambda': 0.11807827311923218, 'min_child_weight': 5, 'gamma': 0.13688634765875174}. Best is trial 2 with value: 0.49384729822835427.\n",
      "🔄 Starting Trial 6 out of 15\n",
      "✅ Trial 6 completed in 7.1s\n",
      "   MAPE: 0.6903, MAE: 77.1605, RMSE: 120.7617\n",
      "[I 2025-01-30 13:26:39,203] Trial 5 finished with value: 0.6902788147058586 and parameters: {'n_estimators': 366, 'learning_rate': 0.019929315219518774, 'max_depth': 16, 'subsample': 0.8674275833508148, 'colsample_bytree': 0.5214013300809219, 'reg_alpha': 0.5864066958672743, 'reg_lambda': 2.633084376049268, 'min_child_weight': 6, 'gamma': 3.051021879934173}. Best is trial 2 with value: 0.49384729822835427.\n",
      "🔄 Starting Trial 7 out of 15\n",
      "✅ Trial 7 completed in 2.9s\n",
      "   MAPE: 0.7347, MAE: 80.7682, RMSE: 120.0548\n",
      "[I 2025-01-30 13:26:42,099] Trial 6 finished with value: 0.7346758200023835 and parameters: {'n_estimators': 157, 'learning_rate': 0.03011831189696193, 'max_depth': 11, 'subsample': 0.8132847245378575, 'colsample_bytree': 0.6023218731100288, 'reg_alpha': 0.88475967496178, 'reg_lambda': 2.211612218064528, 'min_child_weight': 3, 'gamma': 0.301509607164272}. Best is trial 2 with value: 0.49384729822835427.\n",
      "🔄 Starting Trial 8 out of 15\n",
      "✅ Trial 8 completed in 2.4s\n",
      "   MAPE: 1.1287, MAE: 114.9067, RMSE: 159.9749\n",
      "[I 2025-01-30 13:26:44,526] Trial 7 finished with value: 1.1286907731365425 and parameters: {'n_estimators': 202, 'learning_rate': 0.0245127127304165, 'max_depth': 4, 'subsample': 0.5157720153501034, 'colsample_bytree': 0.5704038118742857, 'reg_alpha': 2.8843772598706834, 'reg_lambda': 2.6323858247868017, 'min_child_weight': 2, 'gamma': 3.1938001992329506}. Best is trial 2 with value: 0.49384729822835427.\n",
      "🔄 Starting Trial 9 out of 15\n",
      "✅ Trial 9 completed in 7.8s\n",
      "   MAPE: 0.6199, MAE: 74.6019, RMSE: 121.4384\n",
      "[I 2025-01-30 13:26:52,348] Trial 8 finished with value: 0.6198550822657629 and parameters: {'n_estimators': 402, 'learning_rate': 0.03004745460257961, 'max_depth': 13, 'subsample': 0.9358534914780019, 'colsample_bytree': 0.5239938651387029, 'reg_alpha': 0.12606000530894915, 'reg_lambda': 0.1344639470179681, 'min_child_weight': 7, 'gamma': 2.6408720023314807}. Best is trial 2 with value: 0.49384729822835427.\n",
      "🔄 Starting Trial 10 out of 15\n",
      "✅ Trial 10 completed in 2.7s\n",
      "   MAPE: 0.5243, MAE: 73.5543, RMSE: 118.8696\n",
      "[I 2025-01-30 13:26:55,087] Trial 9 finished with value: 0.524256780203395 and parameters: {'n_estimators': 200, 'learning_rate': 0.024427311034822152, 'max_depth': 8, 'subsample': 0.5020937597192587, 'colsample_bytree': 0.8278542807918776, 'reg_alpha': 0.8300949465336388, 'reg_lambda': 0.4369626475582433, 'min_child_weight': 7, 'gamma': 4.899118764676277}. Best is trial 2 with value: 0.49384729822835427.\n",
      "🔄 Starting Trial 11 out of 15\n",
      "✅ Trial 11 completed in 1.5s\n",
      "   MAPE: 0.4860, MAE: 72.4366, RMSE: 119.2466\n",
      "[I 2025-01-30 13:26:56,601] Trial 10 finished with value: 0.4859836545384457 and parameters: {'n_estimators': 499, 'learning_rate': 0.16795930695649613, 'max_depth': 8, 'subsample': 0.6900371913840349, 'colsample_bytree': 0.674042831529599, 'reg_alpha': 9.306670679632813, 'reg_lambda': 0.9157015542315128, 'min_child_weight': 1, 'gamma': 0.959069323184222}. Best is trial 10 with value: 0.4859836545384457.\n",
      "🔄 Starting Trial 12 out of 15\n",
      "✅ Trial 12 completed in 1.6s\n",
      "   MAPE: 0.4840, MAE: 72.3886, RMSE: 118.8324\n",
      "[I 2025-01-30 13:26:58,169] Trial 11 finished with value: 0.48404383528174444 and parameters: {'n_estimators': 498, 'learning_rate': 0.1317538755958626, 'max_depth': 8, 'subsample': 0.6718151110275, 'colsample_bytree': 0.6808988822240054, 'reg_alpha': 9.999495135116257, 'reg_lambda': 0.8914800287483365, 'min_child_weight': 1, 'gamma': 1.0349073492086378}. Best is trial 11 with value: 0.48404383528174444.\n",
      "🔄 Starting Trial 13 out of 15\n",
      "✅ Trial 13 completed in 2.1s\n",
      "   MAPE: 0.5549, MAE: 74.2715, RMSE: 118.7216\n",
      "[I 2025-01-30 13:27:00,311] Trial 12 finished with value: 0.5549072510136214 and parameters: {'n_estimators': 475, 'learning_rate': 0.16105153030730873, 'max_depth': 7, 'subsample': 0.6830564048949852, 'colsample_bytree': 0.6545482280179679, 'reg_alpha': 9.768635297906089, 'reg_lambda': 0.7035735456249832, 'min_child_weight': 1, 'gamma': 1.0089987492160324}. Best is trial 11 with value: 0.48404383528174444.\n",
      "🔄 Starting Trial 14 out of 15\n",
      "✅ Trial 14 completed in 1.7s\n",
      "   MAPE: 0.4655, MAE: 73.4004, RMSE: 120.9276\n",
      "[I 2025-01-30 13:27:02,032] Trial 13 finished with value: 0.4654791538551022 and parameters: {'n_estimators': 499, 'learning_rate': 0.19778851749512344, 'max_depth': 11, 'subsample': 0.6489641178002785, 'colsample_bytree': 0.655315372397282, 'reg_alpha': 3.0543253555971366, 'reg_lambda': 1.021231997226768, 'min_child_weight': 1, 'gamma': 1.1318890719176682}. Best is trial 13 with value: 0.4654791538551022.\n",
      "🔄 Starting Trial 15 out of 15\n",
      "✅ Trial 15 completed in 2.3s\n",
      "   MAPE: 0.4122, MAE: 71.3145, RMSE: 120.3774\n",
      "[I 2025-01-30 13:27:04,296] Trial 14 finished with value: 0.41220602696209513 and parameters: {'n_estimators': 403, 'learning_rate': 0.10554195700542933, 'max_depth': 11, 'subsample': 0.6043146963145078, 'colsample_bytree': 0.8605305287890477, 'reg_alpha': 2.826701499971443, 'reg_lambda': 0.29056594232473903, 'min_child_weight': 3, 'gamma': 4.107574494727228}. Best is trial 14 with value: 0.41220602696209513.\n",
      "\n",
      "🏆 Best Trial Results:\n",
      "MAPE: 0.4122\n",
      "Best Parameters: {'n_estimators': 403, 'learning_rate': 0.10554195700542933, 'max_depth': 11, 'subsample': 0.6043146963145078, 'colsample_bytree': 0.8605305287890477, 'reg_alpha': 2.826701499971443, 'reg_lambda': 0.29056594232473903, 'min_child_weight': 3, 'gamma': 4.107574494727228, 'enable_categorical': True, 'device': 'cpu', 'tree_method': 'auto'}\n",
      "Final MAPE: 0.4955\n"
     ]
    }
   ],
   "source": [
    "def objective(trial, X_train, X_test, y_train, y_test, total_trials):\n",
    "    \"\"\"\n",
    "    Optuna objective function for XGBoost hyperparameter optimization with GPU support.\n",
    "    \n",
    "    Parameters:\n",
    "    trial: Optuna trial object\n",
    "    X_train, X_test, y_train, y_test: Training and test data\n",
    "    total_trials: Total number of trials to run\n",
    "    \"\"\"\n",
    "    trial_start = time.time()\n",
    "    print(f\"🔄 Starting Trial {trial.number + 1} out of {total_trials}\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    device = \"gpu\" if gpu_available else \"cpu\"\n",
    "    tree_method = \"hist\" if gpu_available else \"auto\"\n",
    "    \n",
    "    # Define hyperparameters with reasonable ranges\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 18),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 10.0, log=True),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 7),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"enable_categorical\": True,\n",
    "        \"device\": device,\n",
    "        \"tree_method\": tree_method\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Convert data to device-appropriate format\n",
    "        if device == \"gpu\":\n",
    "            X_train_device = xgb.DMatrix(X_train, y_train)\n",
    "            X_test_device = xgb.DMatrix(X_test)\n",
    "        else:\n",
    "            X_train_device = X_train\n",
    "            X_test_device = X_test\n",
    "        \n",
    "        # Train the model with early stopping\n",
    "        model = xgb.XGBRegressor(\n",
    "            **param,\n",
    "            early_stopping_rounds=50,\n",
    "            eval_metric=['mae', 'mape']\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train_device, y_train,\n",
    "            eval_set=[(X_test_device, y_test)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        preds = model.predict(X_test_device)\n",
    "        mape = mean_absolute_percentage_error(y_test, preds)\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        mae = np.mean(np.abs(y_test - preds))\n",
    "        rmse = np.sqrt(np.mean((y_test - preds) ** 2))\n",
    "        \n",
    "        # Log results\n",
    "        trial.set_user_attr('mae', mae)\n",
    "        trial.set_user_attr('rmse', rmse)\n",
    "        trial.set_user_attr('n_estimators_used', model.best_iteration or param['n_estimators'])\n",
    "        \n",
    "        trial_time = time.time() - trial_start\n",
    "        print(f\"✅ Trial {trial.number + 1} completed in {trial_time:.1f}s\")\n",
    "        print(f\"   MAPE: {mape:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        return mape\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Trial {trial.number + 1} failed: {str(e)}\")\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "def run_optimization(X_train, X_test, y_train, y_test, n_trials=5):\n",
    "    \"\"\"\n",
    "    Run the complete optimization process and return the best model.\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    \n",
    "    try:\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, X_train, X_test, y_train, y_test, n_trials),\n",
    "            n_trials=n_trials,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Get best parameters and create final model\n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            \"enable_categorical\": True,\n",
    "            \"device\": \"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "            \"tree_method\": \"hist\" if torch.cuda.is_available() else \"auto\"\n",
    "        })\n",
    "        \n",
    "        best_model = xgb.XGBRegressor(**best_params)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"\\n🏆 Best Trial Results:\")\n",
    "        print(f\"MAPE: {study.best_value:.4f}\")\n",
    "        print(\"Best Parameters:\", best_params)\n",
    "        \n",
    "        return best_model, study\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️ Optimization interrupted by user\")\n",
    "        return None, study\n",
    "\n",
    "\n",
    "best_model, study = run_optimization(X_train, X_test, y_train, y_test, n_trials=15)\n",
    "\n",
    "# Make predictions with best model\n",
    "if best_model is not None:\n",
    "    predictions = best_model.predict(X_test)\n",
    "    final_mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "    print(f\"Final MAPE: {final_mape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:29,039] A new study created in memory with name: no-name-bce3ed69-8dfb-4024-a095-ea4cd7b19467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting CatBoost Trial 1 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:30,858] Trial 0 finished with value: 5.761763350471805 and parameters: {'iterations': 162, 'learning_rate': 0.044684647833184404, 'depth': 4, 'l2_leaf_reg': 1.1228124114010449e-05, 'bootstrap_type': 'Poisson', 'subsample': 0.6933973263584963}. Best is trial 0 with value: 5.761763350471805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 1 - MAPE: 5.7618\n",
      "🔄 Starting CatBoost Trial 2 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:32,626] Trial 1 finished with value: 0.758483910016802 and parameters: {'iterations': 184, 'learning_rate': 0.10594707210259802, 'depth': 5, 'l2_leaf_reg': 2.307408401822667, 'bootstrap_type': 'Bayesian'}. Best is trial 1 with value: 0.758483910016802.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 2 - MAPE: 0.7585\n",
      "🔄 Starting CatBoost Trial 3 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:33,979] Trial 2 finished with value: 1.101382593525486 and parameters: {'iterations': 60, 'learning_rate': 0.04776096522144243, 'depth': 8, 'l2_leaf_reg': 8.164537473156187e-05, 'bootstrap_type': 'Bayesian'}. Best is trial 1 with value: 0.758483910016802.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 3 - MAPE: 1.1014\n",
      "🔄 Starting CatBoost Trial 4 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:35,490] Trial 3 finished with value: 1.3900359300491965 and parameters: {'iterations': 135, 'learning_rate': 0.06794533139142842, 'depth': 4, 'l2_leaf_reg': 0.0002230791257605212, 'bootstrap_type': 'Bernoulli', 'subsample': 0.8102858161663142}. Best is trial 1 with value: 0.758483910016802.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 4 - MAPE: 1.3900\n",
      "🔄 Starting CatBoost Trial 5 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:37,045] Trial 4 finished with value: 2.0103806707526735 and parameters: {'iterations': 193, 'learning_rate': 0.19401912783843703, 'depth': 8, 'l2_leaf_reg': 0.8629926583656474, 'bootstrap_type': 'Poisson', 'subsample': 0.6940003044606055}. Best is trial 1 with value: 0.758483910016802.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 5 - MAPE: 2.0104\n",
      "🔄 Starting CatBoost Trial 6 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:38,396] Trial 5 finished with value: 1.5022718844657394 and parameters: {'iterations': 106, 'learning_rate': 0.18500102184735737, 'depth': 3, 'l2_leaf_reg': 0.001549272032359343, 'bootstrap_type': 'Bernoulli', 'subsample': 0.5627388280933792}. Best is trial 1 with value: 0.758483910016802.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 6 - MAPE: 1.5023\n",
      "🔄 Starting CatBoost Trial 7 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:40,194] Trial 6 finished with value: 0.6697986264916028 and parameters: {'iterations': 193, 'learning_rate': 0.18845413235963085, 'depth': 6, 'l2_leaf_reg': 0.00013928841186477446, 'bootstrap_type': 'Bayesian'}. Best is trial 6 with value: 0.6697986264916028.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 7 - MAPE: 0.6698\n",
      "🔄 Starting CatBoost Trial 8 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:41,768] Trial 7 finished with value: 0.520101167310979 and parameters: {'iterations': 60, 'learning_rate': 0.11691105284005349, 'depth': 10, 'l2_leaf_reg': 0.009049856394900944, 'bootstrap_type': 'Bernoulli', 'subsample': 0.8296052439965167}. Best is trial 7 with value: 0.520101167310979.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 8 - MAPE: 0.5201\n",
      "🔄 Starting CatBoost Trial 9 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:43,095] Trial 8 finished with value: 1.4645648627721588 and parameters: {'iterations': 113, 'learning_rate': 0.13092346288055617, 'depth': 3, 'l2_leaf_reg': 4.506046415936257, 'bootstrap_type': 'Bayesian'}. Best is trial 7 with value: 0.520101167310979.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 9 - MAPE: 1.4646\n",
      "🔄 Starting CatBoost Trial 10 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:44,539] Trial 9 finished with value: 3.919196537754801 and parameters: {'iterations': 151, 'learning_rate': 0.09045150203545754, 'depth': 9, 'l2_leaf_reg': 0.015530845470108441, 'bootstrap_type': 'Poisson', 'subsample': 0.5585043511535143}. Best is trial 7 with value: 0.520101167310979.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 10 - MAPE: 3.9192\n",
      "🔄 Starting CatBoost Trial 11 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:46,041] Trial 10 finished with value: 4.450045074503487 and parameters: {'iterations': 54, 'learning_rate': 0.013339418787621404, 'depth': 10, 'l2_leaf_reg': 0.11471196213205336, 'bootstrap_type': 'Bernoulli', 'subsample': 0.9953296353836631}. Best is trial 7 with value: 0.520101167310979.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 11 - MAPE: 4.4500\n",
      "🔄 Starting CatBoost Trial 12 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:47,459] Trial 11 finished with value: 0.707625214137372 and parameters: {'iterations': 87, 'learning_rate': 0.1478551359005292, 'depth': 6, 'l2_leaf_reg': 0.005917376183424007, 'bootstrap_type': 'Bernoulli', 'subsample': 0.8899419312639916}. Best is trial 7 with value: 0.520101167310979.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 12 - MAPE: 0.7076\n",
      "🔄 Starting CatBoost Trial 13 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:48,907] Trial 12 finished with value: 0.6996844244626544 and parameters: {'iterations': 85, 'learning_rate': 0.1568642089197354, 'depth': 7, 'l2_leaf_reg': 0.000575608167805663, 'bootstrap_type': 'Bayesian'}. Best is trial 7 with value: 0.520101167310979.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 13 - MAPE: 0.6997\n",
      "🔄 Starting CatBoost Trial 14 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:50,594] Trial 13 finished with value: 0.6381902557516516 and parameters: {'iterations': 161, 'learning_rate': 0.11598963194637892, 'depth': 6, 'l2_leaf_reg': 0.06149086812777784, 'bootstrap_type': 'Bayesian'}. Best is trial 7 with value: 0.520101167310979.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 14 - MAPE: 0.6382\n",
      "🔄 Starting CatBoost Trial 15 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:53,078] Trial 14 finished with value: 0.4800543401323701 and parameters: {'iterations': 163, 'learning_rate': 0.11086158657051515, 'depth': 10, 'l2_leaf_reg': 0.10145051274409958, 'bootstrap_type': 'Bernoulli', 'subsample': 0.8460945388297807}. Best is trial 14 with value: 0.4800543401323701.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 15 - MAPE: 0.4801\n",
      "🔄 Starting CatBoost Trial 16 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:55,428] Trial 15 finished with value: 0.4660510668770283 and parameters: {'iterations': 141, 'learning_rate': 0.07855653550292105, 'depth': 10, 'l2_leaf_reg': 0.28974240354090314, 'bootstrap_type': 'Bernoulli', 'subsample': 0.8223911554361101}. Best is trial 15 with value: 0.4660510668770283.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 16 - MAPE: 0.4661\n",
      "🔄 Starting CatBoost Trial 17 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:57,476] Trial 16 finished with value: 0.5163720517360089 and parameters: {'iterations': 136, 'learning_rate': 0.08022591237538328, 'depth': 9, 'l2_leaf_reg': 0.2960908855305207, 'bootstrap_type': 'Bernoulli', 'subsample': 0.9267306335447771}. Best is trial 15 with value: 0.4660510668770283.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 17 - MAPE: 0.5164\n",
      "🔄 Starting CatBoost Trial 18 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:11:59,726] Trial 17 finished with value: 0.5196331259927156 and parameters: {'iterations': 173, 'learning_rate': 0.06264327227592452, 'depth': 9, 'l2_leaf_reg': 9.997840624760636, 'bootstrap_type': 'Bernoulli', 'subsample': 0.7576954104418185}. Best is trial 15 with value: 0.4660510668770283.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 18 - MAPE: 0.5196\n",
      "🔄 Starting CatBoost Trial 19 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:12:02,023] Trial 18 finished with value: 0.47729492174162236 and parameters: {'iterations': 142, 'learning_rate': 0.08613977832966405, 'depth': 10, 'l2_leaf_reg': 0.06718657027636453, 'bootstrap_type': 'Bernoulli', 'subsample': 0.8699093973862461}. Best is trial 15 with value: 0.4660510668770283.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 19 - MAPE: 0.4773\n",
      "🔄 Starting CatBoost Trial 20 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:12:03,826] Trial 19 finished with value: 1.6374133236815083 and parameters: {'iterations': 125, 'learning_rate': 0.0176703447634151, 'depth': 8, 'l2_leaf_reg': 0.5264587797180386, 'bootstrap_type': 'Bernoulli', 'subsample': 0.9481743667052187}. Best is trial 15 with value: 0.4660510668770283.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed CatBoost Trial 20 - MAPE: 1.6374\n"
     ]
    }
   ],
   "source": [
    "#Catboost\n",
    "def objective_cb(trial):\n",
    "    print(f\"🔄 Starting CatBoost Trial {trial.number + 1} out of {20}\") \n",
    "\n",
    "    param = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 50, 200),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-5, 10.0, log=True),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"Poisson\"]),\n",
    "        \"task_type\": \"GPU\",  # Enable GPU usage\n",
    "        \"devices\": \"0\"\n",
    "    }\n",
    "\n",
    "    # Ensure subsample is only used when bootstrap_type supports it\n",
    "    if param[\"bootstrap_type\"] in [\"Bernoulli\", \"Poisson\"]:\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "\n",
    "    model = cb.CatBoostRegressor(**param, verbose=0)\n",
    "    model.fit(X_train.values, y_train.values)\n",
    "\n",
    "    preds = model.predict(X_test.values)\n",
    "    mape = mean_absolute_percentage_error(y_test.values, preds)\n",
    "\n",
    "    print(f\"✅ Completed CatBoost Trial {trial.number + 1} - MAPE: {mape:.4f}\")\n",
    "    return mape\n",
    "\n",
    "# Run Optuna study for CatBoost\n",
    "study_cb = optuna.create_study(direction=\"minimize\")\n",
    "study_cb.optimize(objective_cb, n_trials=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 13:12:42,108] A new study created in memory with name: no-name-de34b6ee-e60d-44c7-89f0-dd7defe93482\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting Trial 1 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.002900 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:43,331] Trial 0 finished with value: 0.9630100416045284 and parameters: {'n_estimators': 132, 'learning_rate': 0.17973640337222455, 'max_depth': 5, 'subsample': 0.7390640953956777, 'colsample_bytree': 0.5095748735011991, 'reg_alpha': 8.67217534768859, 'reg_lambda': 3.415559690065175}. Best is trial 0 with value: 0.9630100416045284.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 1 - MAPE: 0.9630\n",
      "🔄 Starting Trial 2 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.003103 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:44,561] Trial 1 finished with value: 0.7604220570572331 and parameters: {'n_estimators': 111, 'learning_rate': 0.22506840477401333, 'max_depth': 17, 'subsample': 0.7392942651703599, 'colsample_bytree': 0.6023245479688216, 'reg_alpha': 5.147891649111764, 'reg_lambda': 4.836699035597948}. Best is trial 1 with value: 0.7604220570572331.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 2 - MAPE: 0.7604\n",
      "🔄 Starting Trial 3 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.002553 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:45,694] Trial 2 finished with value: 0.7957007372154906 and parameters: {'n_estimators': 97, 'learning_rate': 0.11969157999999878, 'max_depth': 15, 'subsample': 0.584748618153723, 'colsample_bytree': 0.6567049489011965, 'reg_alpha': 9.974308828904162, 'reg_lambda': 7.960358837343217}. Best is trial 1 with value: 0.7604220570572331.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 3 - MAPE: 0.7957\n",
      "🔄 Starting Trial 4 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.002597 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:47,144] Trial 3 finished with value: 0.6065866588829986 and parameters: {'n_estimators': 138, 'learning_rate': 0.22418582554962127, 'max_depth': 10, 'subsample': 0.6565770057127198, 'colsample_bytree': 0.9068963039651707, 'reg_alpha': 8.73546777527677, 'reg_lambda': 6.985788935249929}. Best is trial 3 with value: 0.6065866588829986.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 4 - MAPE: 0.6066\n",
      "🔄 Starting Trial 5 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.002551 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:48,530] Trial 4 finished with value: 0.9289021843952893 and parameters: {'n_estimators': 122, 'learning_rate': 0.033660590917891954, 'max_depth': 15, 'subsample': 0.8085816939391042, 'colsample_bytree': 0.6459200956646893, 'reg_alpha': 2.5763204327420834, 'reg_lambda': 4.705258897324793}. Best is trial 3 with value: 0.6065866588829986.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 5 - MAPE: 0.9289\n",
      "🔄 Starting Trial 6 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.002866 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:50,286] Trial 5 finished with value: 0.822479535972599 and parameters: {'n_estimators': 174, 'learning_rate': 0.06922498972469861, 'max_depth': 10, 'subsample': 0.6355530413189407, 'colsample_bytree': 0.6340325015439885, 'reg_alpha': 5.572788803143161, 'reg_lambda': 6.386155065509}. Best is trial 3 with value: 0.6065866588829986.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 6 - MAPE: 0.8225\n",
      "🔄 Starting Trial 7 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.002957 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:51,478] Trial 6 finished with value: 2.783584337065326 and parameters: {'n_estimators': 87, 'learning_rate': 0.016796958645432437, 'max_depth': 16, 'subsample': 0.6528106233864368, 'colsample_bytree': 0.8653951635214276, 'reg_alpha': 6.2367367536542755, 'reg_lambda': 9.497285126153825}. Best is trial 3 with value: 0.6065866588829986.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 7 - MAPE: 2.7836\n",
      "🔄 Starting Trial 8 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.002626 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:52,331] Trial 7 finished with value: 0.6985495601552493 and parameters: {'n_estimators': 58, 'learning_rate': 0.15040286817980245, 'max_depth': 11, 'subsample': 0.9363367695015437, 'colsample_bytree': 0.9629887982861549, 'reg_alpha': 1.0674123493202792, 'reg_lambda': 1.8933373659302226}. Best is trial 3 with value: 0.6065866588829986.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 8 - MAPE: 0.6985\n",
      "🔄 Starting Trial 9 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.002978 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:54,112] Trial 8 finished with value: 0.7412851270831936 and parameters: {'n_estimators': 176, 'learning_rate': 0.07816724339470138, 'max_depth': 18, 'subsample': 0.732612901611728, 'colsample_bytree': 0.6457539343314707, 'reg_alpha': 5.044459070569575, 'reg_lambda': 4.410484230093717}. Best is trial 3 with value: 0.6065866588829986.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 9 - MAPE: 0.7413\n",
      "🔄 Starting Trial 10 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.002633 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:55,178] Trial 9 finished with value: 0.810579322749385 and parameters: {'n_estimators': 93, 'learning_rate': 0.15298366127772037, 'max_depth': 7, 'subsample': 0.8015543902330206, 'colsample_bytree': 0.609898182382735, 'reg_alpha': 5.282920867823123, 'reg_lambda': 1.008040619964252}. Best is trial 3 with value: 0.6065866588829986.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 10 - MAPE: 0.8106\n",
      "🔄 Starting Trial 11 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.002558 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:55,945] Trial 10 finished with value: 1.1575031868629426 and parameters: {'n_estimators': 153, 'learning_rate': 0.2988180557536507, 'max_depth': 3, 'subsample': 0.5118127806602255, 'colsample_bytree': 0.8461254811791169, 'reg_alpha': 8.323328496555018, 'reg_lambda': 7.288857499771286}. Best is trial 3 with value: 0.6065866588829986.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 11 - MAPE: 1.1575\n",
      "🔄 Starting Trial 12 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.005490 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:56,896] Trial 11 finished with value: 0.6112761559183805 and parameters: {'n_estimators': 65, 'learning_rate': 0.2353763036668258, 'max_depth': 11, 'subsample': 0.9648637096594116, 'colsample_bytree': 0.9960148122552503, 'reg_alpha': 0.5030084421767285, 'reg_lambda': 0.7033734080827174}. Best is trial 3 with value: 0.6065866588829986.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 12 - MAPE: 0.6113\n",
      "🔄 Starting Trial 13 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.002549 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:57,780] Trial 12 finished with value: 0.6835385772036359 and parameters: {'n_estimators': 55, 'learning_rate': 0.2462416765642797, 'max_depth': 11, 'subsample': 0.9955776042880919, 'colsample_bytree': 0.9903371802648336, 'reg_alpha': 0.31136925826521433, 'reg_lambda': 0.37941997598598576}. Best is trial 3 with value: 0.6065866588829986.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 13 - MAPE: 0.6835\n",
      "🔄 Starting Trial 14 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.002521 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:12:59,212] Trial 13 finished with value: 0.5725024331660209 and parameters: {'n_estimators': 147, 'learning_rate': 0.23424463481755628, 'max_depth': 8, 'subsample': 0.8875371783564429, 'colsample_bytree': 0.9168418132201327, 'reg_alpha': 3.1587316403649535, 'reg_lambda': 6.60413658740096}. Best is trial 13 with value: 0.5725024331660209.\n",
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 14 - MAPE: 0.5725\n",
      "🔄 Starting Trial 15 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 305\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 6 dense feature groups (1.49 MB) transferred to GPU in 0.002537 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 764.895544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-01-30 13:13:00,692] Trial 14 finished with value: 0.6272658484723265 and parameters: {'n_estimators': 153, 'learning_rate': 0.29054710249897914, 'max_depth': 8, 'subsample': 0.8838130029188263, 'colsample_bytree': 0.8782809063565071, 'reg_alpha': 3.229055379589036, 'reg_lambda': 6.204606509106463}. Best is trial 13 with value: 0.5725024331660209.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed Trial 15 - MAPE: 0.6273\n"
     ]
    }
   ],
   "source": [
    "#lightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "def objective_lgb(trial):\n",
    "    print(f\"🔄 Starting Trial {trial.number + 1} out of {20}\")\n",
    "\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 18),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 10.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 10.0),\n",
    "        \"device\": \"gpu\",  # Enable GPU acceleration\n",
    "        \"boosting_type\": \"gbdt\"\n",
    "    }\n",
    "\n",
    "    # Train the model\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "    model.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # Predict & compute MAPE\n",
    "    preds = model.predict(X_test.values)\n",
    "    mape = mean_absolute_percentage_error(y_test.values, preds)\n",
    "    \n",
    "    print(f\"✅ Completed Trial {trial.number + 1} - MAPE: {mape:.4f}\")\n",
    "\n",
    "    return mape\n",
    "\n",
    "# Run Optuna study for LightGBM\n",
    "study_lgb = optuna.create_study(direction=\"minimize\")\n",
    "study_lgb.optimize(objective_lgb, n_trials=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_checker = initialize_holiday_detection(blind)\n",
    "blind['is_holiday'] =blind.apply(holiday_checker, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230130</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Discount Stickers</td>\n",
       "      <td>Holographic Goose</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230131</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Discount Stickers</td>\n",
       "      <td>Kaggle</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id        date country              store            product  \\\n",
       "0  230130  2017-01-01  Canada  Discount Stickers  Holographic Goose   \n",
       "1  230131  2017-01-01  Canada  Discount Stickers             Kaggle   \n",
       "\n",
       "   is_holiday  \n",
       "0        True  \n",
       "1        True  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blind.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trial = feature_eng(blind)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_trial)\n",
    "submission_df = pd.DataFrame({\"predictions\": predictions})\n",
    "submission_df = pd.merge(blind['id'], \n",
    "         submission_df,\n",
    "         left_index=True,\n",
    "         right_index=True)\n",
    "submission_file = submission_df.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 731.68994,  672.5833 ,  554.0756 , ..., 1995.6849 , 1142.4535 ,\n",
       "       1370.7294 ], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
