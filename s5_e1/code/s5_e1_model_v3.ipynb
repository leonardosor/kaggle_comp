{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'altair'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-932e9ff14de4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0maltair\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'altair'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import optuna\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import holidays \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import catboost as cb\n",
    "from scalecast.Forecaster import Forecaster\n",
    "\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "\n",
    "import time\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "sklearn.set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "blind = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_holiday(row):\n",
    "    \"\"\"\n",
    "    Determines if a given date is a holiday in the specified country.\n",
    "    Optimized for Canada, Finland, and Italy.\n",
    "    \n",
    "    \"\"\"\n",
    "    country = row['country']\n",
    "    date = pd.to_datetime(row['date']).date()  # Convert to date only\n",
    "\n",
    "    country_mapping = {\n",
    "        'Canada': holidays.CA(),\n",
    "        'Finland': holidays.FI(),\n",
    "        'Italy': holidays.IT(), \n",
    "        'Kenya': holidays.KE(),\n",
    "        'Norway': holidays.NO(),\n",
    "        'Singapore': holidays.SG(),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get the holidays object for the country\n",
    "        if country in country_mapping:\n",
    "            country_holidays = country_mapping[country]\n",
    "            return date in country_holidays\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking holiday for {country} on {date}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create a cache of holiday objects to improve performance\n",
    "def initialize_holiday_detection(df):\n",
    "    \"\"\"\n",
    "    Initialize holiday detection by creating holiday objects for all unique years in the dataset.\n",
    "    This improves performance by avoiding repeated creation of holiday objects.\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame containing 'date' column\n",
    "    \"\"\"\n",
    "    years = pd.to_datetime(df['date']).dt.year.unique()\n",
    "    holiday_cache = {\n",
    "        'Canada': {year: holidays.CA(years=year) for year in years},\n",
    "        'Finland': {year: holidays.FI(years=year) for year in years},\n",
    "        'Italy': {year: holidays.IT(years=year) for year in years}, \n",
    "        'Kenya': {year: holidays.KE(years=year) for year in years},\n",
    "        'Norway': {year: holidays.NO(years=year) for year in years},\n",
    "        'Singapore': {year: holidays.SG(years=year) for year in years}\n",
    "\n",
    "    }\n",
    "    \n",
    "    def is_holiday_cached(row):\n",
    "        country = row['country']\n",
    "        date = pd.to_datetime(row['date']).date()\n",
    "        year = date.year\n",
    "        \n",
    "        try:\n",
    "            if country in holiday_cache:\n",
    "                return date in holiday_cache[country][year]\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking holiday for {country} on {date}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return is_holiday_cached\n",
    "\n",
    "# Example usage:\n",
    "# First initialize the cached version\n",
    "holiday_checker = initialize_holiday_detection(train)\n",
    "\n",
    "# Then apply it to create the holiday flag\n",
    "train['is_holiday'] = train.apply(holiday_checker, axis=1)\n",
    "\n",
    "holiday_checker = initialize_holiday_detection(blind)\n",
    "blind['is_holiday'] = blind.apply(holiday_checker, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df):\n",
    "    df = df.copy()\n",
    "    try: \n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['day_of_year'] = df['date'].dt.dayofyear\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['year'] = df['date'].dt.year\n",
    "    except: pass\n",
    "\n",
    "    df[[\"country\", \"store\", \"product\", \"is_holiday\"]] = df[[\"country\", \"store\", \"product\", \"is_holiday\"]].astype(\"category\")\n",
    "    # Encode categorical features\n",
    "    # Drop missing values and irrelevant columns\n",
    "    imputer = SimpleImputer(strategy=\"mean\")  # Change to \"mean\", \"most_frequent\", or \"constant\"\n",
    "    scaler = MinMaxScaler()\n",
    "    try: \n",
    "        df = df.set_index('id')\n",
    "    except: pass\n",
    "    try:\n",
    "        df = df.drop(columns=['date'])  # Dropping columns that are not useful for modeling\n",
    "    except: pass\n",
    "    # Separate features and target\n",
    "    try: \n",
    "        df[\"num_sold\"] = imputer.fit_transform(df[[\"num_sold\"]])\n",
    "       # df[\"num_sold\"] = scaler.fit_transform(df[[\"num_sold\"]])\n",
    "    except: pass\n",
    "    print(df.info())\n",
    "    \n",
    "    try: \n",
    "        X = df.drop(columns=['num_sold'])\n",
    "        #X = pd.get_dummies(X)\n",
    "    except:\n",
    "        X=df#pd.get_dummies(df)\n",
    "    try:\n",
    "        y = df['num_sold']\n",
    "    except:\n",
    "        y = None\n",
    "    # Scale features\n",
    "    return df, X, y\n",
    "\n",
    "# Feature engineering\n",
    "df, X, y = feature_eng(train)\n",
    "blind = feature_eng(blind)[1]\n",
    "\n",
    "# Splitting the data\n",
    "SPLIT = 0.85\n",
    "split_index = int(SPLIT * len(X))\n",
    "\n",
    "X_train = X[:split_index]\n",
    "y_train = y[:split_index]\n",
    "X_test = X[split_index:]\n",
    "y_test = y[split_index:]\n",
    "\n",
    "# Verify shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def objective(trial, X_train, X_test, y_train, y_test, total_trials):\n",
    "    \"\"\"\n",
    "    Optuna objective function for XGBoost hyperparameter optimization with GPU support.\n",
    "    \"\"\"\n",
    "    trial_start = time.time()\n",
    "    print(f\"üîÑ Starting Trial {trial.number + 1} out of {total_trials}\")\n",
    "    \n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    device = \"gpu\" if gpu_available else \"cpu\"\n",
    "    tree_method = \"hist\" if gpu_available else \"auto\"\n",
    "    \n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 18),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 10.0, log=True),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 7),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"enable_categorical\": True,\n",
    "        \"device\": device,\n",
    "        \"tree_method\": tree_method\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        X_train.info()\n",
    "        y_train.info()\n",
    "        X_train_device = xgb.DMatrix(X_train, y_train)\n",
    "        X_test_device = xgb.DMatrix(X_test)\n",
    "       \n",
    "        \n",
    "        model = xgb.XGBRegressor(\n",
    "            **param,\n",
    "            early_stopping_rounds=50,\n",
    "            eval_metric=['mae', 'mape']\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train_device, y_train,\n",
    "            eval_set=[(X_test_device, y_test)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(X_test_device)\n",
    "        mape = mean_absolute_percentage_error(y_test, preds)\n",
    "        mae = np.mean(np.abs(y_test - preds))\n",
    "        rmse = np.sqrt(np.mean((y_test - preds) ** 2))\n",
    "        \n",
    "        trial.set_user_attr('mae', mae)\n",
    "        trial.set_user_attr('rmse', rmse)\n",
    "        trial.set_user_attr('n_estimators_used', model.best_iteration or param['n_estimators'])\n",
    "        \n",
    "        trial_time = time.time() - trial_start\n",
    "        print(f\"‚úÖ Trial {trial.number + 1} completed in {trial_time:.1f}s\")\n",
    "        print(f\"   MAPE: {mape:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        return mape\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Trial {trial.number + 1} failed: {str(e)}\")\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "def run_optimization(X_train, X_test, y_train, y_test, n_trials=2):\n",
    "    \"\"\"\n",
    "    Run the complete optimization process and return the best model.\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    \n",
    "    try:\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, X_train, X_test, y_train, y_test, n_trials),\n",
    "            n_trials=n_trials,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            \"enable_categorical\": True,\n",
    "            \"device\": \"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "            \"tree_method\": \"hist\" if torch.cuda.is_available() else \"auto\"\n",
    "        })\n",
    "        \n",
    "        best_model = xgb.XGBRegressor(**best_params)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"\\nüèÜ Best Trial Results:\")\n",
    "        print(f\"MAPE: {study.best_value:.4f}\")\n",
    "        print(\"Best Parameters:\", best_params)\n",
    "        \n",
    "        return best_model, study\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è Optimization interrupted by user\")\n",
    "        return None, study\n",
    "\n",
    "def train_country_models(df, X_train, X_test, y_train, y_test, n_trials=2):\n",
    "    \"\"\"\n",
    "    Train separate XGBoost models for each country using Optuna optimization.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    countries = df['country'].unique()\n",
    "    country_models = {}\n",
    "    \n",
    "    for country in countries:\n",
    "        country_start_time = time.time()\n",
    "        print(f\"\\nüåç Training model for {country}\")\n",
    "        print(\"=*\" * 50)\n",
    "        \n",
    "        train_country_mask = df.loc[X_train.index]['country'] == country\n",
    "        test_country_mask = df.loc[X_test.index]['country'] == country\n",
    "        \n",
    "        X_train_country = X_train[train_country_mask]\n",
    "        X_test_country = X_test[test_country_mask]\n",
    "        y_train_country = y_train[train_country_mask]\n",
    "        y_test_country = y_test[test_country_mask]\n",
    "        \n",
    "        if len(X_train_country) < 10:\n",
    "            print(f\"‚ö†Ô∏è Insufficient data for {country}. Skipping...\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            best_model, study = run_optimization(\n",
    "                X_train_country, \n",
    "                X_test_country,\n",
    "                y_train_country, \n",
    "                y_test_country,\n",
    "                n_trials=n_trials\n",
    "            )\n",
    "            \n",
    "            if best_model is not None:\n",
    "                predictions = best_model.predict(X_test_country)\n",
    "                final_mape = mean_absolute_percentage_error(y_test_country, predictions)\n",
    "                \n",
    "                country_models[country] = {\n",
    "                    'model': best_model,\n",
    "                    'study': study,\n",
    "                    'mape': final_mape,\n",
    "                    'n_train_samples': len(X_train_country),\n",
    "                    'n_test_samples': len(X_test_country),\n",
    "                    'best_parameters': study.best_params,\n",
    "                    'training_time': time.time() - country_start_time\n",
    "                }\n",
    "                \n",
    "                print(f\"\\nüìä Results for {country}:\")\n",
    "                print(f\"Training samples: {len(X_train_country)}\")\n",
    "                print(f\"Test samples: {len(X_test_country)}\")\n",
    "                print(f\"Final MAPE: {final_mape:.4f}\")\n",
    "                print(f\"Training time: {country_models[country]['training_time']:.1f}s\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error training model for {country}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n‚è±Ô∏è Total training time: {total_time:.1f}s\")\n",
    "    \n",
    "    print(\"\\nüìë Overall Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    summary_data = []\n",
    "    for country, results in country_models.items():\n",
    "        summary_data.append({\n",
    "            'Country': country,\n",
    "            'MAPE': results['mape'],\n",
    "            'Training Samples': results['n_train_samples'],\n",
    "            'Test Samples': results['n_test_samples'],\n",
    "            'Training Time (s)': results['training_time']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df)\n",
    "    \n",
    "    return country_models\n",
    "\n",
    "def predict_with_country_models(X_new, country_models, country_column):\n",
    "    \"\"\"\n",
    "    Make predictions using country-specific models on new, unlabeled data.\n",
    "    \n",
    "    Parameters:\n",
    "    X_new: DataFrame containing features for prediction (without target)\n",
    "    country_models: Dictionary of trained models from train_country_models()\n",
    "    country_column: String or array containing country values for each row\n",
    "    \n",
    "    Returns:\n",
    "    array: Predictions for each row\n",
    "    \"\"\"\n",
    "    predictions = np.zeros(len(X_new))\n",
    "    \n",
    "    for country, model_dict in country_models.items():\n",
    "        # Get indices for this country\n",
    "        country_mask = country_column == country\n",
    "        \n",
    "        if country_mask.any():\n",
    "            # Get predictions for this country\n",
    "            country_data = X_new[country_mask]\n",
    "            predictions[country_mask] = model_dict['model'].predict(country_data)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Train the models\n",
    "country_models = train_country_models(\n",
    "    df=df,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    n_trials=20\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict_with_country_models(\n",
    "    blind, \n",
    "    country_models, \n",
    "    blind['country'])   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions_df = pd.DataFrame({\"num_sold\": predictions})\n",
    "predictions_df = predictions_df.reset_index().rename(columns={\"index\": \"on\"})\n",
    "\n",
    "temp_blind = blind.copy()\n",
    "temp_blind['on'] = range(1, len(temp_blind) + 1)\n",
    "\n",
    "submissions = temp_blind.merge(predictions_df, on='on', how='left')\n",
    "submissions = submissions[[\"num_sold\", \"id\"]]\n",
    "submissions = submissions.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
