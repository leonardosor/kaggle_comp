{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import optuna\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import holidays \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import catboost as cb\n",
    "from scalecast.Forecaster import Forecaster\n",
    "\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "\n",
    "import time\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "sklearn.set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{datetime.date(2025, 1, 1): \"New Year's Day\", datetime.date(2025, 5, 26): 'Memorial Day', datetime.date(2025, 6, 19): 'Juneteenth National Independence Day', datetime.date(2025, 7, 4): 'Independence Day', datetime.date(2025, 9, 1): 'Labor Day', datetime.date(2025, 11, 11): 'Veterans Day', datetime.date(2025, 11, 27): 'Thanksgiving', datetime.date(2025, 12, 25): 'Christmas Day', datetime.date(2025, 1, 20): 'Martin Luther King Jr. Day', datetime.date(2025, 2, 17): \"Washington's Birthday\", datetime.date(2025, 10, 13): 'Columbus Day'}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "holidays.US(years=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 230130 entries, 0 to 230129\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   id        230130 non-null  int64  \n",
      " 1   date      230130 non-null  object \n",
      " 2   country   230130 non-null  object \n",
      " 3   store     230130 non-null  object \n",
      " 4   product   230130 non-null  object \n",
      " 5   num_sold  221259 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 10.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "blind = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_profile = ProfileReport(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_holiday(row):\n",
    "    \"\"\"\n",
    "    Determines if a given date is a holiday in the specified country.\n",
    "    Optimized for Canada, Finland, and Italy.\n",
    "    \n",
    "    Parameters:\n",
    "    row: pandas Series containing 'country' and 'date' columns\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the date is a holiday in the given country, False otherwise\n",
    "    \"\"\"\n",
    "    country = row['country']\n",
    "    date = pd.to_datetime(row['date']).date()  # Convert to date only\n",
    "\n",
    "    country_mapping = {\n",
    "        'Canada': holidays.CA(),\n",
    "        'Finland': holidays.FI(),\n",
    "        'Italy': holidays.IT(), \n",
    "        'Kenya': holidays.KE(),\n",
    "        'Norway': holidays.NO(),\n",
    "        'Singapore': holidays.SG(),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get the holidays object for the country\n",
    "        if country in country_mapping:\n",
    "            country_holidays = country_mapping[country]\n",
    "            return date in country_holidays\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking holiday for {country} on {date}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create a cache of holiday objects to improve performance\n",
    "def initialize_holiday_detection(df):\n",
    "    \"\"\"\n",
    "    Initialize holiday detection by creating holiday objects for all unique years in the dataset.\n",
    "    This improves performance by avoiding repeated creation of holiday objects.\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame containing 'date' column\n",
    "    \"\"\"\n",
    "    years = pd.to_datetime(df['date']).dt.year.unique()\n",
    "    holiday_cache = {\n",
    "        'Canada': {year: holidays.CA(years=year) for year in years},\n",
    "        'Finland': {year: holidays.FI(years=year) for year in years},\n",
    "        'Italy': {year: holidays.IT(years=year) for year in years}, \n",
    "        'Kenya': {year: holidays.KE(years=year) for year in years},\n",
    "        'Norway': {year: holidays.NO(years=year) for year in years},\n",
    "        'Singapore': {year: holidays.SG(years=year) for year in years}\n",
    "\n",
    "    }\n",
    "    \n",
    "    def is_holiday_cached(row):\n",
    "        country = row['country']\n",
    "        date = pd.to_datetime(row['date']).date()\n",
    "        year = date.year\n",
    "        \n",
    "        try:\n",
    "            if country in holiday_cache:\n",
    "                return date in holiday_cache[country][year]\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking holiday for {country} on {date}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return is_holiday_cached\n",
    "\n",
    "# Example usage:\n",
    "# First initialize the cached version\n",
    "holiday_checker = initialize_holiday_detection(train)\n",
    "\n",
    "# Then apply it to create the holiday flag\n",
    "train['is_holiday'] = train.apply(holiday_checker, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (195610, 19)\n",
      "y_train shape: (195610,)\n",
      "X_test shape: (34520, 19)\n",
      "y_test shape: (34520,)\n"
     ]
    }
   ],
   "source": [
    "def feature_eng(df):\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Extract date features\n",
    "    \n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['day_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "\n",
    "    # Encode categorical features\n",
    "    \n",
    "\n",
    "\n",
    "    # Drop missing values and irrelevant columns\n",
    "    imputer = SimpleImputer(strategy=\"median\")  # Change to \"mean\", \"most_frequent\", or \"constant\"\n",
    "    \n",
    "    df = df.set_index('date')\n",
    "    df = df.drop(columns=['id'])  # Dropping columns that are not useful for modeling\n",
    "    # Separate features and target\n",
    "    try: \n",
    "        df[\"num_sold\"] = imputer.fit_transform(df[[\"num_sold\"]])\n",
    "    except: pass\n",
    "    try: \n",
    "        X = df.drop(columns=['num_sold'])\n",
    "        X = pd.get_dummies(X)\n",
    "\n",
    "    except:\n",
    "        X=pd.get_dummies(df)\n",
    "    try:\n",
    "        y = df['num_sold']\n",
    "    except:\n",
    "        y = None\n",
    "    # Scale features\n",
    "    #X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    return df, X,  y\n",
    "\n",
    "# Feature engineering\n",
    "df, X, y = feature_eng(train)\n",
    "\n",
    "# Splitting the data\n",
    "SPLIT = 0.85\n",
    "split_index = int(SPLIT * len(X))\n",
    "\n",
    "X_train = X[:split_index]\n",
    "y_train = y[:split_index]\n",
    "X_test = X[split_index:]\n",
    "y_test = y[split_index:]\n",
    "\n",
    "# Verify shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 239.6190\n",
      "MAPE: 3.2049\n",
      "Mean Squared Error: 109541.0906\n",
      "R-squared: 0.6742\n",
      "Coefficients: [ 3.61205001e+01  6.86410539e-02  2.38087569e+01 -2.99911700e+00\n",
      " -6.35812851e+00  6.81236178e+01 -4.00209836e+00 -2.08989529e+02\n",
      " -6.63598094e+02  7.13789478e+02  9.46766248e+01 -3.12797164e+02\n",
      "  2.25828312e+02  8.69688512e+01 -4.79094090e+02  4.96165201e+02\n",
      "  2.75448520e+02 -1.93938551e+02 -9.85810797e+01]\n",
      "Intercept: 13489.12723115343\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "# Train the model using training data\n",
    "model.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test.values)\n",
    "\n",
    "# Evaluate the model using common metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}\")\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "# Optionally: You can get the coefficients and intercept of the linear regression model\n",
    "print(f\"Coefficients: {model.coef_}\")\n",
    "print(f\"Intercept: {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 12:28:57,527] A new study created in memory with name: no-name-f7ba9455-e99d-46a3-99c2-7388e9a638fd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Starting Trial 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [12:28:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-01-30 12:28:58,865] Trial 0 finished with value: 0.6605830691622739 and parameters: {'n_estimators': 271, 'learning_rate': 0.18793297710581836, 'max_depth': 5, 'subsample': 0.9718692862623313, 'colsample_bytree': 0.5352327519748985, 'reg_alpha': 1.934644828288758, 'reg_lambda': 1.2046216208166824}. Best is trial 0 with value: 0.6605830691622739.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Trial 1 - MAPE: 0.6606\n",
      "ðŸ”„ Starting Trial 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 12:29:19,601] Trial 1 finished with value: 0.477071541779303 and parameters: {'n_estimators': 479, 'learning_rate': 0.10395377307741967, 'max_depth': 14, 'subsample': 0.6228710088366072, 'colsample_bytree': 0.5208097233791955, 'reg_alpha': 3.07527433979948, 'reg_lambda': 2.603865463210192}. Best is trial 1 with value: 0.477071541779303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Trial 2 - MAPE: 0.4771\n",
      "ðŸ”„ Starting Trial 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 12:30:48,822] Trial 2 finished with value: 0.38192155099568004 and parameters: {'n_estimators': 386, 'learning_rate': 0.1653669021468973, 'max_depth': 18, 'subsample': 0.9801241608589688, 'colsample_bytree': 0.6629652720938222, 'reg_alpha': 8.64959558453139, 'reg_lambda': 4.190135197292421}. Best is trial 2 with value: 0.38192155099568004.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Trial 3 - MAPE: 0.3819\n",
      "ðŸ”„ Starting Trial 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 12:30:57,615] Trial 3 finished with value: 0.5377703983136883 and parameters: {'n_estimators': 157, 'learning_rate': 0.04014636574478453, 'max_depth': 18, 'subsample': 0.8693792119211561, 'colsample_bytree': 0.5298794949801255, 'reg_alpha': 7.073489627090692, 'reg_lambda': 7.4992558111335414}. Best is trial 2 with value: 0.38192155099568004.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Trial 4 - MAPE: 0.5378\n",
      "ðŸ”„ Starting Trial 5 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-30 12:31:05,843] Trial 4 failed with parameters: {'n_estimators': 387, 'learning_rate': 0.09062129748555271, 'max_depth': 16, 'subsample': 0.5762699469681487, 'colsample_bytree': 0.955998208847619, 'reg_alpha': 3.2919930528970487, 'reg_lambda': 1.9956472774028984} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_17116\\70811098.py\", line 20, in objective\n",
      "    model.fit(X_train.values, y_train.values)\n",
      "  File \"c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\xgboost\\sklearn.py\", line 1108, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\xgboost\\core.py\", line 2101, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-01-30 12:31:05,858] Trial 4 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Run Optuna study\u001b[39;00m\n\u001b[0;32m     32\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# âœ… Use explicit n_trials here\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[25], line 20\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Predict & compute MAPE\u001b[39;00m\n\u001b[0;32m     23\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\xgboost\\sklearn.py:1108\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1105\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m-> 1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\xgboost\\core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2100\u001b[0m     _check_call(\n\u001b[1;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2104\u001b[0m     )\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    print(f\"ðŸ”„ Starting Trial {trial.number + 1} out of {5}\")  # Replace 20 with your actual n_trials\n",
    "\n",
    "    # Define hyperparameters\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 18),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 10.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 10.0),\n",
    "        \"enable_categorical\": True,  # Enable categorical features\n",
    "        \"device\": \"gpu\",  # Ensure GPU usage\n",
    "        \"tree_method\": \"hist\"  # Recommended for GPU acceleration in XGBoost 2.0+\n",
    "    }\n",
    "\n",
    "    # Train the model\n",
    "    model = xgb.XGBRegressor(**param)\n",
    "    model.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # Predict & compute MAPE\n",
    "    preds = model.predict(X_test.values)\n",
    "    mape = mean_absolute_percentage_error(y_test.values, preds)\n",
    "    #blind_preds = model.predict(blind)\n",
    "    #blind_mape = mean_absolute_percentage_error(y_train, blind_preds)\n",
    "    print(f\"âœ… Completed Trial {trial.number + 1} - MAPE: {mape:.4f}\")\n",
    "\n",
    "    return mape\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=5)  # âœ… Use explicit n_trials here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 12:45:51,078] A new study created in memory with name: no-name-0b7516cc-1c37-44c9-bc62-43a6b4e6c8c0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0c54cd6ca5497087c8683e2fd6cc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Starting Trial 1 out of 15\n",
      "âœ… Trial 1 completed in 3.0s\n",
      "   MAPE: 1.2029, MAE: 92.0556, RMSE: 116.8164\n",
      "[I 2025-01-30 12:45:54,116] Trial 0 finished with value: 1.2028849659894678 and parameters: {'n_estimators': 226, 'learning_rate': 0.0140989437140282, 'max_depth': 7, 'subsample': 0.9358988117079101, 'colsample_bytree': 0.5696372790613986, 'reg_alpha': 0.5184694706783463, 'reg_lambda': 0.1568892955675274, 'min_child_weight': 7, 'gamma': 0.5291305330285995}. Best is trial 0 with value: 1.2028849659894678.\n",
      "ðŸ”„ Starting Trial 2 out of 15\n",
      "âœ… Trial 2 completed in 4.6s\n",
      "   MAPE: 0.3540, MAE: 53.5593, RMSE: 94.2412\n",
      "[I 2025-01-30 12:45:58,733] Trial 1 finished with value: 0.3540062660549229 and parameters: {'n_estimators': 312, 'learning_rate': 0.05001841889601309, 'max_depth': 12, 'subsample': 0.5563484558428962, 'colsample_bytree': 0.731307914749396, 'reg_alpha': 0.6842499592232841, 'reg_lambda': 0.7576098080671874, 'min_child_weight': 4, 'gamma': 1.992806812179479}. Best is trial 1 with value: 0.3540062660549229.\n",
      "ðŸ”„ Starting Trial 3 out of 15\n",
      "âœ… Trial 3 completed in 11.3s\n",
      "   MAPE: 0.4103, MAE: 57.1398, RMSE: 99.7242\n",
      "[I 2025-01-30 12:46:10,036] Trial 2 finished with value: 0.4102754665002679 and parameters: {'n_estimators': 467, 'learning_rate': 0.0828344563533445, 'max_depth': 16, 'subsample': 0.5635656573064833, 'colsample_bytree': 0.6462777825275012, 'reg_alpha': 2.1661919022314993, 'reg_lambda': 1.3283213441813306, 'min_child_weight': 1, 'gamma': 2.6058305640375283}. Best is trial 1 with value: 0.3540062660549229.\n",
      "ðŸ”„ Starting Trial 4 out of 15\n",
      "âœ… Trial 4 completed in 3.6s\n",
      "   MAPE: 0.4224, MAE: 53.5200, RMSE: 90.8515\n",
      "[I 2025-01-30 12:46:13,663] Trial 3 finished with value: 0.42238067409915764 and parameters: {'n_estimators': 441, 'learning_rate': 0.02811494330523702, 'max_depth': 8, 'subsample': 0.7959839606845458, 'colsample_bytree': 0.7541540198016973, 'reg_alpha': 7.584734607906624, 'reg_lambda': 5.20290607214662, 'min_child_weight': 7, 'gamma': 3.736593437218059}. Best is trial 1 with value: 0.3540062660549229.\n",
      "ðŸ”„ Starting Trial 5 out of 15\n",
      "âœ… Trial 5 completed in 3.6s\n",
      "   MAPE: 0.4337, MAE: 53.6698, RMSE: 92.3875\n",
      "[I 2025-01-30 12:46:17,245] Trial 4 finished with value: 0.43373603891934803 and parameters: {'n_estimators': 222, 'learning_rate': 0.03511940292564574, 'max_depth': 9, 'subsample': 0.8270616939538012, 'colsample_bytree': 0.6258346524585863, 'reg_alpha': 0.5008085407030493, 'reg_lambda': 0.3316331237922289, 'min_child_weight': 3, 'gamma': 2.884788734737161}. Best is trial 1 with value: 0.3540062660549229.\n",
      "ðŸ”„ Starting Trial 6 out of 15\n",
      "âœ… Trial 6 completed in 6.5s\n",
      "   MAPE: 0.3712, MAE: 54.8682, RMSE: 96.2780\n",
      "[I 2025-01-30 12:46:23,734] Trial 5 finished with value: 0.37122692945260094 and parameters: {'n_estimators': 212, 'learning_rate': 0.04603248923752734, 'max_depth': 15, 'subsample': 0.7550051073633564, 'colsample_bytree': 0.6802423329352927, 'reg_alpha': 1.4003631882439218, 'reg_lambda': 1.497659688678918, 'min_child_weight': 6, 'gamma': 0.45292920561905814}. Best is trial 1 with value: 0.3540062660549229.\n",
      "ðŸ”„ Starting Trial 7 out of 15\n",
      "âœ… Trial 7 completed in 2.7s\n",
      "   MAPE: 1.0702, MAE: 120.0265, RMSE: 162.0969\n",
      "[I 2025-01-30 12:46:26,466] Trial 6 finished with value: 1.0702169289915704 and parameters: {'n_estimators': 242, 'learning_rate': 0.05252921313644291, 'max_depth': 3, 'subsample': 0.6180073282416212, 'colsample_bytree': 0.5668829970342733, 'reg_alpha': 0.9340687547111459, 'reg_lambda': 0.10934952963369796, 'min_child_weight': 3, 'gamma': 3.7027459962339817}. Best is trial 1 with value: 0.3540062660549229.\n",
      "ðŸ”„ Starting Trial 8 out of 15\n",
      "âœ… Trial 8 completed in 4.5s\n",
      "   MAPE: 0.7039, MAE: 68.9313, RMSE: 106.2200\n",
      "[I 2025-01-30 12:46:30,981] Trial 7 finished with value: 0.7039214380225424 and parameters: {'n_estimators': 425, 'learning_rate': 0.023993290722783906, 'max_depth': 5, 'subsample': 0.8518360562503908, 'colsample_bytree': 0.628932709685541, 'reg_alpha': 1.102035461575017, 'reg_lambda': 0.1430288873034439, 'min_child_weight': 2, 'gamma': 1.4869174570040544}. Best is trial 1 with value: 0.3540062660549229.\n",
      "ðŸ”„ Starting Trial 9 out of 15\n",
      "âœ… Trial 9 completed in 1.8s\n",
      "   MAPE: 0.4590, MAE: 55.0389, RMSE: 93.0379\n",
      "[I 2025-01-30 12:46:32,765] Trial 8 finished with value: 0.45903800520886073 and parameters: {'n_estimators': 149, 'learning_rate': 0.10584278932376147, 'max_depth': 7, 'subsample': 0.6190536124437415, 'colsample_bytree': 0.8496785509609139, 'reg_alpha': 0.11390230855081532, 'reg_lambda': 0.16528384849066058, 'min_child_weight': 1, 'gamma': 3.7932699139151733}. Best is trial 1 with value: 0.3540062660549229.\n",
      "ðŸ”„ Starting Trial 10 out of 15\n",
      "âœ… Trial 10 completed in 1.6s\n",
      "   MAPE: 3.0135, MAE: 178.0310, RMSE: 224.6598\n",
      "[I 2025-01-30 12:46:34,401] Trial 9 finished with value: 3.0135266649600676 and parameters: {'n_estimators': 114, 'learning_rate': 0.013886509377546999, 'max_depth': 9, 'subsample': 0.5974073755177091, 'colsample_bytree': 0.5748389339889106, 'reg_alpha': 5.024125077871788, 'reg_lambda': 0.33598147677009194, 'min_child_weight': 2, 'gamma': 0.024760095302948648}. Best is trial 1 with value: 0.3540062660549229.\n",
      "ðŸ”„ Starting Trial 11 out of 15\n",
      "âœ… Trial 11 completed in 1.8s\n",
      "   MAPE: 0.3110, MAE: 52.3538, RMSE: 93.2203\n",
      "[I 2025-01-30 12:46:36,233] Trial 10 finished with value: 0.3110134773297456 and parameters: {'n_estimators': 346, 'learning_rate': 0.1832632715005114, 'max_depth': 13, 'subsample': 0.5074612559848591, 'colsample_bytree': 0.9500461345605775, 'reg_alpha': 0.13280224668633706, 'reg_lambda': 4.100176594212785, 'min_child_weight': 5, 'gamma': 4.9068870221412055}. Best is trial 10 with value: 0.3110134773297456.\n",
      "ðŸ”„ Starting Trial 12 out of 15\n",
      "âœ… Trial 12 completed in 2.2s\n",
      "   MAPE: 0.3222, MAE: 52.6795, RMSE: 93.5004\n",
      "[I 2025-01-30 12:46:38,401] Trial 11 finished with value: 0.32220669530153023 and parameters: {'n_estimators': 343, 'learning_rate': 0.15042484819671503, 'max_depth': 13, 'subsample': 0.5457764615667469, 'colsample_bytree': 0.9904146633382842, 'reg_alpha': 0.11840132951043128, 'reg_lambda': 9.931330223073502, 'min_child_weight': 5, 'gamma': 4.774299832892038}. Best is trial 10 with value: 0.3110134773297456.\n",
      "ðŸ”„ Starting Trial 13 out of 15\n",
      "âœ… Trial 13 completed in 4.3s\n",
      "   MAPE: 0.3200, MAE: 52.7091, RMSE: 93.6474\n",
      "[I 2025-01-30 12:46:42,714] Trial 12 finished with value: 0.32000822692560704 and parameters: {'n_estimators': 331, 'learning_rate': 0.19803523322160413, 'max_depth': 13, 'subsample': 0.5011842803135178, 'colsample_bytree': 0.9688793485508268, 'reg_alpha': 0.10149483790633203, 'reg_lambda': 8.968822257683861, 'min_child_weight': 5, 'gamma': 4.6576539269829516}. Best is trial 10 with value: 0.3110134773297456.\n",
      "ðŸ”„ Starting Trial 14 out of 15\n",
      "âœ… Trial 14 completed in 5.1s\n",
      "   MAPE: 0.3156, MAE: 53.4692, RMSE: 96.3135\n",
      "[I 2025-01-30 12:46:47,814] Trial 13 finished with value: 0.31561173572207185 and parameters: {'n_estimators': 364, 'learning_rate': 0.17062501527604546, 'max_depth': 18, 'subsample': 0.6823949790437864, 'colsample_bytree': 0.9852291390267226, 'reg_alpha': 0.21489667486033373, 'reg_lambda': 3.6225850535792317, 'min_child_weight': 5, 'gamma': 4.944306087463137}. Best is trial 10 with value: 0.3110134773297456.\n",
      "ðŸ”„ Starting Trial 15 out of 15\n",
      "âœ… Trial 15 completed in 7.1s\n",
      "   MAPE: 0.3130, MAE: 53.9529, RMSE: 97.2976\n",
      "[I 2025-01-30 12:46:54,915] Trial 14 finished with value: 0.3129680890212726 and parameters: {'n_estimators': 372, 'learning_rate': 0.09858711745468877, 'max_depth': 18, 'subsample': 0.6702129520222082, 'colsample_bytree': 0.898246277202442, 'reg_alpha': 0.23505584038418817, 'reg_lambda': 3.0662454397068473, 'min_child_weight': 5, 'gamma': 4.880911955898022}. Best is trial 10 with value: 0.3110134773297456.\n",
      "\n",
      "ðŸ† Best Trial Results:\n",
      "MAPE: 0.3110\n",
      "Best Parameters: {'n_estimators': 346, 'learning_rate': 0.1832632715005114, 'max_depth': 13, 'subsample': 0.5074612559848591, 'colsample_bytree': 0.9500461345605775, 'reg_alpha': 0.13280224668633706, 'reg_lambda': 4.100176594212785, 'min_child_weight': 5, 'gamma': 4.9068870221412055, 'enable_categorical': True, 'device': 'cpu', 'tree_method': 'auto'}\n",
      "Final MAPE: 0.3817\n"
     ]
    }
   ],
   "source": [
    "def objective(trial, X_train, X_test, y_train, y_test, total_trials):\n",
    "    \"\"\"\n",
    "    Optuna objective function for XGBoost hyperparameter optimization with GPU support.\n",
    "    \n",
    "    Parameters:\n",
    "    trial: Optuna trial object\n",
    "    X_train, X_test, y_train, y_test: Training and test data\n",
    "    total_trials: Total number of trials to run\n",
    "    \"\"\"\n",
    "    trial_start = time.time()\n",
    "    print(f\"ðŸ”„ Starting Trial {trial.number + 1} out of {total_trials}\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    device = \"gpu\" if gpu_available else \"cpu\"\n",
    "    tree_method = \"hist\" if gpu_available else \"auto\"\n",
    "    \n",
    "    # Define hyperparameters with reasonable ranges\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 18),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 10.0, log=True),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 7),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"enable_categorical\": True,\n",
    "        \"device\": device,\n",
    "        \"tree_method\": tree_method\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Convert data to device-appropriate format\n",
    "        if device == \"gpu\":\n",
    "            X_train_device = xgb.DMatrix(X_train, y_train)\n",
    "            X_test_device = xgb.DMatrix(X_test)\n",
    "        else:\n",
    "            X_train_device = X_train\n",
    "            X_test_device = X_test\n",
    "        \n",
    "        # Train the model with early stopping\n",
    "        model = xgb.XGBRegressor(\n",
    "            **param,\n",
    "            early_stopping_rounds=50,\n",
    "            eval_metric=['mae', 'mape']\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train_device, y_train,\n",
    "            eval_set=[(X_test_device, y_test)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        preds = model.predict(X_test_device)\n",
    "        mape = mean_absolute_percentage_error(y_test, preds)\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        mae = np.mean(np.abs(y_test - preds))\n",
    "        rmse = np.sqrt(np.mean((y_test - preds) ** 2))\n",
    "        \n",
    "        # Log results\n",
    "        trial.set_user_attr('mae', mae)\n",
    "        trial.set_user_attr('rmse', rmse)\n",
    "        trial.set_user_attr('n_estimators_used', model.best_iteration or param['n_estimators'])\n",
    "        \n",
    "        trial_time = time.time() - trial_start\n",
    "        print(f\"âœ… Trial {trial.number + 1} completed in {trial_time:.1f}s\")\n",
    "        print(f\"   MAPE: {mape:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        return mape\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Trial {trial.number + 1} failed: {str(e)}\")\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "def run_optimization(X_train, X_test, y_train, y_test, n_trials=5):\n",
    "    \"\"\"\n",
    "    Run the complete optimization process and return the best model.\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    \n",
    "    try:\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, X_train, X_test, y_train, y_test, n_trials),\n",
    "            n_trials=n_trials,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Get best parameters and create final model\n",
    "        best_params = study.best_params\n",
    "        best_params.update({\n",
    "            \"enable_categorical\": True,\n",
    "            \"device\": \"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "            \"tree_method\": \"hist\" if torch.cuda.is_available() else \"auto\"\n",
    "        })\n",
    "        \n",
    "        best_model = xgb.XGBRegressor(**best_params)\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"\\nðŸ† Best Trial Results:\")\n",
    "        print(f\"MAPE: {study.best_value:.4f}\")\n",
    "        print(\"Best Parameters:\", best_params)\n",
    "        \n",
    "        return best_model, study\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâš ï¸ Optimization interrupted by user\")\n",
    "        return None, study\n",
    "\n",
    "\n",
    "best_model, study = run_optimization(X_train, X_test, y_train, y_test, n_trials=15)\n",
    "\n",
    "# Make predictions with best model\n",
    "if best_model is not None:\n",
    "    predictions = best_model.predict(X_test)\n",
    "    final_mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "    print(f\"Final MAPE: {final_mape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 10:32:17,429] A new study created in memory with name: no-name-ca9d989e-e7d7-4a3b-ba50-cfc0531a8b02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Starting CatBoost Trial 1 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 10:32:19,864] Trial 0 finished with value: 0.5642048639613763 and parameters: {'iterations': 110, 'learning_rate': 0.07761315610356374, 'depth': 10, 'l2_leaf_reg': 1.287032171603146e-05, 'bootstrap_type': 'Bernoulli', 'subsample': 0.993958007682485}. Best is trial 0 with value: 0.5642048639613763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed CatBoost Trial 1 - MAPE: 0.5642\n",
      "ðŸ”„ Starting CatBoost Trial 2 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 10:32:21,459] Trial 1 finished with value: 1.2054574011116603 and parameters: {'iterations': 183, 'learning_rate': 0.045263313626500004, 'depth': 3, 'l2_leaf_reg': 0.0005504960257199449, 'bootstrap_type': 'Bayesian'}. Best is trial 0 with value: 0.5642048639613763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed CatBoost Trial 2 - MAPE: 1.2055\n",
      "ðŸ”„ Starting CatBoost Trial 3 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 10:32:23,139] Trial 2 finished with value: 0.6216740509737476 and parameters: {'iterations': 108, 'learning_rate': 0.11889387523844663, 'depth': 6, 'l2_leaf_reg': 0.1149033202782803, 'bootstrap_type': 'Bayesian'}. Best is trial 0 with value: 0.5642048639613763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed CatBoost Trial 3 - MAPE: 0.6217\n",
      "ðŸ”„ Starting CatBoost Trial 4 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 10:32:24,749] Trial 3 finished with value: 1.3690629686356102 and parameters: {'iterations': 135, 'learning_rate': 0.013700592645527324, 'depth': 5, 'l2_leaf_reg': 7.120787883299445e-05, 'bootstrap_type': 'Poisson', 'subsample': 0.7280860114280876}. Best is trial 0 with value: 0.5642048639613763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed CatBoost Trial 4 - MAPE: 1.3691\n",
      "ðŸ”„ Starting CatBoost Trial 5 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 10:32:26,275] Trial 4 finished with value: 0.6745534215792119 and parameters: {'iterations': 63, 'learning_rate': 0.0703410258316165, 'depth': 8, 'l2_leaf_reg': 0.03814485996533083, 'bootstrap_type': 'Bernoulli', 'subsample': 0.6321310790982253}. Best is trial 0 with value: 0.5642048639613763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed CatBoost Trial 5 - MAPE: 0.6746\n"
     ]
    }
   ],
   "source": [
    "#Catboost\n",
    "def objective_cb(trial):\n",
    "    print(f\"ðŸ”„ Starting CatBoost Trial {trial.number + 1} out of {20}\") \n",
    "\n",
    "    param = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 50, 200),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-5, 10.0, log=True),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"Poisson\"]),\n",
    "        \"task_type\": \"GPU\",  # Enable GPU usage\n",
    "        \"devices\": \"0\"\n",
    "    }\n",
    "\n",
    "    # Ensure subsample is only used when bootstrap_type supports it\n",
    "    if param[\"bootstrap_type\"] in [\"Bernoulli\", \"Poisson\"]:\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "\n",
    "    model = cb.CatBoostRegressor(**param, verbose=0)\n",
    "    model.fit(X_train.values, y_train.values)\n",
    "\n",
    "    preds = model.predict(X_test.values)\n",
    "    mape = mean_absolute_percentage_error(y_test.values, preds)\n",
    "\n",
    "    print(f\"âœ… Completed CatBoost Trial {trial.number + 1} - MAPE: {mape:.4f}\")\n",
    "    return mape\n",
    "\n",
    "# Run Optuna study for CatBoost\n",
    "study_cb = optuna.create_study(direction=\"minimize\")\n",
    "study_cb.optimize(objective_cb, n_trials=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 10:32:53,263] A new study created in memory with name: no-name-59249260-7cad-411a-87c9-b392b930b2cf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Starting Trial 1 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 310\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 7 dense feature groups (1.49 MB) transferred to GPU in 0.004394 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.018130\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 10:32:55,016] Trial 0 finished with value: 0.6661336500835574 and parameters: {'n_estimators': 143, 'learning_rate': 0.05421579704191731, 'max_depth': 5, 'subsample': 0.7714021918077434, 'colsample_bytree': 0.7757299648452833, 'reg_alpha': 6.666265064362723, 'reg_lambda': 7.404644607747894}. Best is trial 0 with value: 0.6661336500835574.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Trial 1 - MAPE: 0.6661\n",
      "ðŸ”„ Starting Trial 2 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 310\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 7 dense feature groups (1.49 MB) transferred to GPU in 0.003377 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.018130\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 10:32:56,419] Trial 1 finished with value: 0.5890301337015383 and parameters: {'n_estimators': 120, 'learning_rate': 0.23917404264091283, 'max_depth': 6, 'subsample': 0.5702624363564109, 'colsample_bytree': 0.5302540189053162, 'reg_alpha': 9.181284904224954, 'reg_lambda': 5.777833963385993}. Best is trial 1 with value: 0.5890301337015383.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Trial 2 - MAPE: 0.5890\n",
      "ðŸ”„ Starting Trial 3 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 310\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 7 dense feature groups (1.49 MB) transferred to GPU in 0.003671 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.018130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 10:32:57,788] Trial 2 finished with value: 0.6306295815582748 and parameters: {'n_estimators': 64, 'learning_rate': 0.1841993373877428, 'max_depth': 17, 'subsample': 0.5835372413642886, 'colsample_bytree': 0.9158238803841554, 'reg_alpha': 4.319373692781404, 'reg_lambda': 1.2383517907348505}. Best is trial 1 with value: 0.5890301337015383.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Trial 3 - MAPE: 0.6306\n",
      "ðŸ”„ Starting Trial 4 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 310\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 7 dense feature groups (1.49 MB) transferred to GPU in 0.003020 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.018130\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 10:32:59,950] Trial 3 finished with value: 0.6655998943569493 and parameters: {'n_estimators': 100, 'learning_rate': 0.20510095694932834, 'max_depth': 15, 'subsample': 0.6958803783553931, 'colsample_bytree': 0.7942416619544808, 'reg_alpha': 8.023697065904198, 'reg_lambda': 1.432326646475349}. Best is trial 1 with value: 0.5890301337015383.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Trial 4 - MAPE: 0.6656\n",
      "ðŸ”„ Starting Trial 5 out of 20\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 310\n",
      "[LightGBM] [Info] Number of data points in the train set: 195610, number of used features: 18\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1660 Ti, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 7 dense feature groups (1.49 MB) transferred to GPU in 0.004067 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.018130\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-30 10:33:01,786] Trial 4 finished with value: 0.6758076443891302 and parameters: {'n_estimators': 135, 'learning_rate': 0.06539363345497198, 'max_depth': 5, 'subsample': 0.7668706791010944, 'colsample_bytree': 0.7899188132034961, 'reg_alpha': 9.583677812457712, 'reg_lambda': 0.7563834387705128}. Best is trial 1 with value: 0.5890301337015383.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed Trial 5 - MAPE: 0.6758\n"
     ]
    }
   ],
   "source": [
    "#lightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "def objective_lgb(trial):\n",
    "    print(f\"ðŸ”„ Starting Trial {trial.number + 1} out of {20}\")\n",
    "\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 18),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 10.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 10.0),\n",
    "        \"device\": \"gpu\",  # Enable GPU acceleration\n",
    "        \"boosting_type\": \"gbdt\"\n",
    "    }\n",
    "\n",
    "    # Train the model\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "    model.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # Predict & compute MAPE\n",
    "    preds = model.predict(X_test.values)\n",
    "    mape = mean_absolute_percentage_error(y_test.values, preds)\n",
    "    \n",
    "    print(f\"âœ… Completed Trial {trial.number + 1} - MAPE: {mape:.4f}\")\n",
    "\n",
    "    return mape\n",
    "\n",
    "# Run Optuna study for LightGBM\n",
    "study_lgb = optuna.create_study(direction=\"minimize\")\n",
    "study_lgb.optimize(objective_lgb, n_trials=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trial = feature_eng(blind)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m holiday_checker \u001b[38;5;241m=\u001b[39m initialize_holiday_detection(blind)\n\u001b[1;32m----> 2\u001b[0m blind[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_holiday\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m\u001b[43mblind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mholiday_checker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[42], line 57\u001b[0m, in \u001b[0;36minitialize_holiday_detection.<locals>.is_holiday_cached\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_holiday_cached\u001b[39m(row):\n\u001b[0;32m     56\u001b[0m     country \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 57\u001b[0m     date \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdate()\n\u001b[0;32m     58\u001b[0m     year \u001b[38;5;241m=\u001b[39m date\u001b[38;5;241m.\u001b[39myear\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1101\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1099\u001b[0m         result \u001b[38;5;241m=\u001b[39m convert_listlike(argc, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1101\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43marg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, np\u001b[38;5;241m.\u001b[39mbool_):\n\u001b[0;32m   1103\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(result)  \u001b[38;5;66;03m# TODO: avoid this kludge.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[0;32m    436\u001b[0m     arg,\n\u001b[0;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    442\u001b[0m )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:479\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    477\u001b[0m     res \u001b[38;5;241m=\u001b[39m Index(result, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM8[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, UTC]\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m--> 479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\pandas\\core\\indexes\\base.py:485\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[1;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__new__\u001b[39m(\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m    477\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    481\u001b[0m     tupleize_cols: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    482\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrange\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RangeIndex\n\u001b[1;32m--> 485\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_extract_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m pandas_dtype(dtype)\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7692\u001b[0m, in \u001b[0;36mmaybe_extract_name\u001b[1;34m(name, obj, cls)\u001b[0m\n\u001b[0;32m   7688\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmaybe_extract_name\u001b[39m(name, obj, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Hashable:\n\u001b[0;32m   7689\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   7690\u001b[0m \u001b[38;5;124;03m    If no name is passed, then extract it from data, validating hashability.\u001b[39;00m\n\u001b[0;32m   7691\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 7692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mIndex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mABCSeries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   7693\u001b[0m         \u001b[38;5;66;03m# Note we don't just check for \"name\" attribute since that would\u001b[39;00m\n\u001b[0;32m   7694\u001b[0m         \u001b[38;5;66;03m#  pick up e.g. dtype.name\u001b[39;00m\n\u001b[0;32m   7695\u001b[0m         name \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m   7697\u001b[0m     \u001b[38;5;66;03m# GH#29069\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\.pyenv\\pyenv-win-venv\\envs\\venv_kaggle\\lib\\site-packages\\pandas\\core\\dtypes\\generic.py:42\u001b[0m, in \u001b[0;36mcreate_pandas_abc_type.<locals>._instancecheck\u001b[1;34m(cls, inst)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(inst, attr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_typ\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m comp\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# https://github.com/python/mypy/issues/1006\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# error: 'classmethod' used with a non-method\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_instancecheck\u001b[39m(\u001b[38;5;28mcls\u001b[39m, inst) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _check(inst) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inst, \u001b[38;5;28mtype\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_subclasscheck\u001b[39m(\u001b[38;5;28mcls\u001b[39m, inst) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Raise instead of returning False\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# This is consistent with default __subclasscheck__ behavior\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "holiday_checker = initialize_holiday_detection(blind)\n",
    "blind['is_holiday'] =blind.apply(holiday_checker, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230130</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Discount Stickers</td>\n",
       "      <td>Holographic Goose</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id        date country              store            product  \\\n",
       "0  230130  2017-01-01  Canada  Discount Stickers  Holographic Goose   \n",
       "\n",
       "   is_holiday  \n",
       "0       False  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blind.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = final_model.predict(X_trial.values)\n",
    "submission_df = pd.DataFrame({\"predictions\": predictions})\n",
    "submission_df = pd.merge(blind['id'], \n",
    "         submission_df,\n",
    "         left_index=True,\n",
    "         right_index=True)\n",
    "submission_file = submission_df.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 603.00525,  822.55347,  717.7928 , ..., 1864.3871 , 1046.0208 ,\n",
       "       1267.8745 ], dtype=float32)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230130</td>\n",
       "      <td>603.005249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230131</td>\n",
       "      <td>822.553467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>230132</td>\n",
       "      <td>717.792786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230133</td>\n",
       "      <td>390.541016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230134</td>\n",
       "      <td>443.878021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98545</th>\n",
       "      <td>328675</td>\n",
       "      <td>401.625610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98546</th>\n",
       "      <td>328676</td>\n",
       "      <td>2251.829102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98547</th>\n",
       "      <td>328677</td>\n",
       "      <td>1864.387085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98548</th>\n",
       "      <td>328678</td>\n",
       "      <td>1046.020752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98549</th>\n",
       "      <td>328679</td>\n",
       "      <td>1267.874512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98550 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  predictions\n",
       "0      230130   603.005249\n",
       "1      230131   822.553467\n",
       "2      230132   717.792786\n",
       "3      230133   390.541016\n",
       "4      230134   443.878021\n",
       "...       ...          ...\n",
       "98545  328675   401.625610\n",
       "98546  328676  2251.829102\n",
       "98547  328677  1864.387085\n",
       "98548  328678  1046.020752\n",
       "98549  328679  1267.874512\n",
       "\n",
       "[98550 rows x 2 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 610.90393,  838.7246 ,  677.8083 , ..., 1953.422  , 1048.2671 ,\n",
       "       1243.5255 ], dtype=float32)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230130</td>\n",
       "      <td>610.903931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230131</td>\n",
       "      <td>838.724609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>230132</td>\n",
       "      <td>677.808289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230133</td>\n",
       "      <td>368.214539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230134</td>\n",
       "      <td>438.093872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98545</th>\n",
       "      <td>328675</td>\n",
       "      <td>374.361359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98546</th>\n",
       "      <td>328676</td>\n",
       "      <td>2303.772705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98547</th>\n",
       "      <td>328677</td>\n",
       "      <td>1953.421997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98548</th>\n",
       "      <td>328678</td>\n",
       "      <td>1048.267090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98549</th>\n",
       "      <td>328679</td>\n",
       "      <td>1243.525513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98550 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  predictions\n",
       "0      230130   610.903931\n",
       "1      230131   838.724609\n",
       "2      230132   677.808289\n",
       "3      230133   368.214539\n",
       "4      230134   438.093872\n",
       "...       ...          ...\n",
       "98545  328675   374.361359\n",
       "98546  328676  2303.772705\n",
       "98547  328677  1953.421997\n",
       "98548  328678  1048.267090\n",
       "98549  328679  1243.525513\n",
       "\n",
       "[98550 rows x 2 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m blind_predition_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit(test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "blind_predition_model = model.fit(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
